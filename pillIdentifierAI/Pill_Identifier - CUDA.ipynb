{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pill Identifier Machine Learning Model and API\n",
    "All pills and tablets have a unique combination of features that allow them to be identified. \n",
    "These features are its color, its shape, and imprints made in front and/or at the back of the drug.\n",
    "The model aims to predict the name of an unknown pill/tablet based on these features.\n",
    "\n",
    "This model will utilize `google/vit-base-patch16-224` for image classification.\n",
    "\n",
    "The dataset is from the U.S. Department of Health's Computational Photography Project for Pill Identification (C3PI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7SczRM2XJyN"
   },
   "source": [
    "# Imports, Declarations, and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:43.358190Z",
     "start_time": "2024-01-04T22:36:38.947503Z"
    },
    "id": "YSVBXMIEXQLa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\adirb\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt     # to plot charts\n",
    "import numpy as np\n",
    "import pandas as pd                 # for data manipulation\n",
    "import cv2                          # for image processing\n",
    "from io import BytesIO\n",
    "from tabulate import tabulate       # to print pretty tables\n",
    "\n",
    "# sklearn imports for metrics and dataset splitting\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras imports for image preprocessing\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# huggingface imports for model building \n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTForImageClassification, TrainingArguments, Trainer, \\\n",
    "  default_data_collator, EarlyStoppingCallback, ViTConfig, AutoImageProcessor, ViTImageProcessor \n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# keras imports for early stoppage and model checkpointing\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from datasets import load_dataset, load_metric, Features, ClassLabel, Array3D, Dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://data.lhncbc.nlm.nih.gov/public/Pills/'\n",
    "directory = \"dataset\"\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeJN4BQ3dF0H"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:45.362505Z",
     "start_time": "2024-01-04T22:36:45.356006Z"
    },
    "id": "WC-oRwKNdOqu"
   },
   "outputs": [],
   "source": [
    "# Function to convert an image file to a tensor\n",
    "def image_to_tensor(image_file):\n",
    "    image = Image.open(image_file)\n",
    "    image = Resize((224, 224))(image)\n",
    "    return ToTensor()(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54DLyRiiXkpH"
   },
   "source": [
    "# Data Acquisition\n",
    "Retrieves the images from our dataset and stores them in memory.\n",
    "Corresponding labels are retrieved, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:46.590066Z",
     "start_time": "2024-01-04T22:36:46.524940Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg9m_GC-zWMu",
    "outputId": "9cb4cc80-cc39-4af9-8cde-1902baa34c7d"
   },
   "outputs": [],
   "source": [
    "# read the csv file with labels\n",
    "csv_file = \"table.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_label\"] = label_encoder.fit_transform(df[\"name\"])\n",
    "\n",
    "# create a dataset from the dataframe\n",
    "image_paths = df[\"rxnavImageFileName\"].values\n",
    "labels = df[\"encoded_label\"].values\n",
    "num_labels = len(df[\"encoded_label\"].unique())\n",
    "print(\"Number of labels:\", num_labels)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_df = pd.DataFrame(list(dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    "\n",
    "# print the first 5 image paths and decoded labels\n",
    "for image, label in dataset.take(5):\n",
    "  print(\"Image:\", image.numpy())\n",
    "  print(\"Label:\", label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  print()\n",
    "\n",
    "# np.save('encoder/encoder.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:47.545335Z",
     "start_time": "2024-01-04T22:36:47.513945Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_images(path, label):\n",
    "  image = tf.io.read_file(directory + '/' + path)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.resize(image, [256, 256])\n",
    "  image /= 255.0 \n",
    "  return image, label\n",
    "\n",
    "dataset = dataset.map(load_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-04T22:36:48.374189Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# display the first 9 images and their labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(dataset.take(9)):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image)\n",
    "  plt.title(label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add datapoints to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing labels: 0\n",
      "Total number of labels after removing missing labels:  133774\n",
      "encoded_label\n",
      "2866    434\n",
      "1659    413\n",
      "175     392\n",
      "2539    379\n",
      "1158    377\n",
      "       ... \n",
      "1635     13\n",
      "1452     13\n",
      "899      13\n",
      "1517     13\n",
      "1453     13\n",
      "Name: count, Length: 3010, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "excel_file = \"directory_consumer_grade_images.xlsx\"\n",
    "df_xls = pd.read_excel(excel_file)\n",
    "\n",
    "# encode the labels\n",
    "df_xls[\"encoded_label\"] = label_encoder.fit_transform(df_xls[\"Name\"])\n",
    "\n",
    "# Check for missing labels\n",
    "missing_labels = df_xls['encoded_label'].isnull().sum()\n",
    "print(\"Number of missing labels:\", missing_labels)\n",
    "\n",
    "# Remove data points without a label\n",
    "df_xls = df_xls.dropna(subset=['encoded_label'])\n",
    "\n",
    "# Check the number of labels again\n",
    "print(\"Total number of labels after removing missing labels: \", len(df_xls[\"encoded_label\"]))\n",
    "\n",
    "# Combine old and new df\n",
    "df_combined = pd.concat([df, df_xls])\n",
    "\n",
    "# Check if the data is imbalanced\n",
    "label_counts = df_combined['encoded_label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Check if the data is imbalanced\n",
    "label_counts = df_combined['encoded_label'].value_counts()\n",
    "\n",
    "# Plot the label counts\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(label_counts, bins=250, alpha=0.5, color='g')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Number of Images per Label')\n",
    "plt.ylabel('Number of Labels')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nm5cEhnXQrY"
   },
   "source": [
    "# Data Augmentation and Preprocessing\n",
    "Because each pill/tablet only has one picture, the data set in itself is not ideal.\n",
    "To improve the quality of the data set, and that of the model, we augment the data.\n",
    "We do this by transforming the image, mimicking how an actual user may take a picture.\n",
    "That is, the image can be brightened, resized, rotated, sheared, cropped, and etc. Other processes are also performed to improve training of the model such as splitting the data into a training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxGoMZM6Xpo9",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Convert column into strings\n",
    "dataset_df[\"image_paths\"] = dataset_df[\"image_paths\"].astype(str)\n",
    "dataset_df[\"labels\"] = dataset_df[\"labels\"].astype(str)\n",
    "\n",
    "print(dataset_df[\"image_paths\"].head())\n",
    "\n",
    "#Splitting dataset into 60/20/20\n",
    "train_df, temp_df = train_test_split(dataset_df, test_size=0.4, random_state=42)\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=1337)\n",
    "\n",
    "# Create the image data generator for the training set\n",
    "imageTrain_data = ImageDataGenerator(\n",
    "    rescale = 1./255.,\n",
    "    rotation_range = 60,\n",
    "    shear_range = 0.3,\n",
    "    zoom_range = 0.5,\n",
    "    vertical_flip = True,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "# Create the image data generator for the evaluation set\n",
    "imageEval_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "eval_generator = imageEval_data.flow_from_dataframe(\n",
    "    dataframe=eval_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the image data generator for the test set\n",
    "imageTest_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "test_generator = imageTest_data.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "#Display example of image augmentation\n",
    "sample_dataframe = train_df.sample(n=1).reset_index(drop=True)\n",
    "sample_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=sample_dataframe,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range (0, 15):\n",
    "  ax = plt.subplot(5, 3, i + 1)\n",
    "  for X_column, Y_column in sample_generator:\n",
    "    plt.imshow(X_column[0])\n",
    "    break\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-ZGIIlGXqWY"
   },
   "source": [
    "# Filtering\n",
    "Using OpenCV, we filter out any artifacts (i.e. background, lens flares, graininess, etc.) and extract the features necessary for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X28VD5zqX9Pu",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def thresholding(img, alpha=0.5):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE to the grayscale image\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2,2))\n",
    "    gray_clahe = clahe.apply(gray)\n",
    "\n",
    "    # Convert the grayscale image back to BGR\n",
    "    img_clahe = cv2.cvtColor(gray_clahe, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Blend the CLAHE image with the original image\n",
    "    result = cv2.addWeighted(img, alpha, img_clahe, 1 - alpha, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_background(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a blur\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out small contours based on area\n",
    "    contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 500]\n",
    "\n",
    "    # Create an empty mask to store the result\n",
    "    mask = np.zeros_like(thresh)\n",
    "\n",
    "    # Draw the contours on the mask\n",
    "    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Perform morphological operations\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "    mask = cv2.dilate(mask,kernel,iterations = 1)\n",
    "\n",
    "    # Invert the mask\n",
    "    mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Bitwise AND the mask and the original image\n",
    "    res = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zW9RMHnXyhu"
   },
   "source": [
    "# Hyperparameter Search\n",
    "To ensure the best set of hyperparameters used by the model, we enable hyperparameter search prior to training the model. This exhaustively searches the best combination of hyperparameters to be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the dataframe into a dataset\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = read_image(image_path)  # Open the image and convert it to a tensor\n",
    "        image = Resize((224, 224), antialias=True)(image)  # Resize the image\n",
    "        return {'pixel_values': image, 'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XICqDptZYCI2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "token = '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'             # Jan's personal access token\n",
    "configuration = ViTConfig()\n",
    "\n",
    "# Select only 100 rows from the training set\n",
    "train_df = train_df.sample(n=100)\n",
    "\n",
    "# Prepend the path to the dataset folder to each file path\n",
    "train_df['image_paths'] = train_df['image_paths'].apply(lambda x: x if x.startswith('dataset') else os.path.join('dataset', x))\n",
    "\n",
    "# Split data into a training set and an evaluation set\n",
    "train_df, eval_df = train_test_split(train_df, test_size=0.2)  # Use 20% of your data for evaluation\n",
    "\n",
    "# Reset the index of the DataFrame to avoid indexing errors\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Convert your images and labels to tensors\n",
    "pixel_values = [image_to_tensor(image_file) for image_file in train_df['image_paths']]\n",
    "labels = train_df['labels'].to_numpy()\n",
    "\n",
    "# Create a dictionary with the pixel values and labels\n",
    "train_data = {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = ImageClassificationDataset(train_df['image_paths'], train_df['labels'].to_numpy())\n",
    "eval_dataset = ImageClassificationDataset(eval_df['image_paths'], eval_df['labels'].to_numpy())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    evaluation_strategy=\"steps\",    \n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    ")\n",
    "\n",
    "\n",
    "def model_init(trial):\n",
    "    num_labels = len(np.unique(train_df['labels'].to_numpy()))\n",
    "    configuration.num_labels = num_labels           # Set the number of output units to match the number of classes\n",
    "    return ViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=configuration,\n",
    "        from_tf=bool(\".ckpt\" in model_name),\n",
    "        cache_dir=model_name,                       # use cache to speed up model loading\n",
    "        token=token,\n",
    "        ignore_mismatched_sizes=True                # ignore image size mismatch errors\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "#Execute hyperparameter search\n",
    "hypersearch = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=8, \n",
    ")\n",
    "\n",
    "\n",
    "print(hypersearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning with Dataset Chunks\n",
    " Explore the efficiency of incremental learning with 3.3TB dataset chunks. Optimize training by iteratively downloading, processing, and removing chunks for improved resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: b'PillProjectDisc69/images/CLLLLUPGIX7J8MP1WWQ9WN4-CO0B5NV.CR2'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc98/images/PRNJ-AXZIQ!HUQKJJBP_DV44ST0KN9.CR2'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc10/images/79U-YY6M1UUR6F127ZMACIWPEEXHLB.JPG'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc11/images/7WVFV5H74!ELFNQ_GUH92E9ERM9P2K.JPG'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc20/images/B4CH0R9B7PEQ6GORRX-8XWOL-_G7W9_.JPG'\n",
      "Label: STRATTERA 10MG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# read the xlsx file\n",
    "xlsx_file = \"./directory_consumer_grade_images.xlsx\"\n",
    "df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "# Iterate over the third column and append to the list\n",
    "image_label_mapping = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row.iloc[2]  \n",
    "    label = row.iloc[4]  \n",
    "    if not image_path.endswith('.WMV'):\n",
    "        image_label_mapping[image_path] = label\n",
    "\n",
    "# get the image paths and labels\n",
    "image_paths = list(image_label_mapping.keys())\n",
    "labels = list(image_label_mapping.values())\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# create a dataset from the dataframe\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_df = pd.DataFrame(list(dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    "\n",
    "# print the first 5 image paths and decoded labels\n",
    "for image, label in dataset.take(5):\n",
    "  print(\"Image:\", image.numpy())\n",
    "  print(\"Label:\", label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n",
      "410\n",
      "410\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets (60/20/20)\n",
    "train_df, temp_df = train_test_split(dataset_df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['image_paths'].values, train_df['labels'].values))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_df['image_paths'].values, val_df['labels'].values))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_df['image_paths'].values, test_df['labels'].values))\n",
    "\n",
    "# Shuffle and batch the datasets\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_df)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load the first batch of data\n",
    "first_batch = next(iter(train_dataset))\n",
    "\n",
    "# Unpack the batch\n",
    "image_paths, labels = first_batch\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(len(image_paths)):\n",
    "    image_path_str = image_paths[i].numpy().decode('utf-8')\n",
    "    image = Image.open(BytesIO(requests.get(DATASET_URL + image_path_str).content))\n",
    "    ax = plt.subplot(8, 8, i + 1) \n",
    "    plt.imshow(image)\n",
    "    plt.title(label_encoder.inverse_transform([labels[i].numpy()])[0])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzwPHUrRYCdM"
   },
   "source": [
    "# Model Training\n",
    "We train the model using the best hyperparameters on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "def load_and_preprocess_images(example):\n",
    "    # Load the image from the file\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = int(example['labels'])\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "# Generate lists of image paths and labels for training dataset\n",
    "train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "train_labels = train_df[\"labels\"].tolist()\n",
    "\n",
    "# Create a dictionary with the image paths and labels\n",
    "train_dict = {'image_paths': train_image_paths, 'labels': train_labels}\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# Repeat the same process for the evaluation and test datasets\n",
    "eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "eval_labels = eval_df[\"labels\"].tolist()\n",
    "eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels}\n",
    "eval_dataset = Dataset.from_dict(eval_dict)\n",
    "eval_dataset = eval_dataset.map(load_and_preprocess_images)\n",
    "eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "test_labels = test_df[\"labels\"].tolist()\n",
    "test_dict = {'image_paths': test_image_paths, 'labels': test_labels}\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_images)\n",
    "test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = num_labels\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=10,  \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    ")\n",
    "\n",
    "# mainTrainer.train()\n",
    "# model.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FQOqnPYEg1"
   },
   "source": [
    "# Model Testing\n",
    "We test the model on the test set to validate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess the images for testing\n",
    "def load_and_preprocess_test_images(example):\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)\n",
    "    label = int(example['labels'])\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_test_images)\n",
    "test_dataset = test_dataset.remove_columns(['image_paths'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_losses = []\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to predictions\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "        # Append predictions and true labels to lists\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_losses.append(loss)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_losses = np.array(all_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "classification_report_str = classification_report(all_labels, all_predictions)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {fscore}\")\n",
    "print(\"Classification Report:\\n\", tabulate([[''] + classification_report_str.split('\\n')[0].split()] + [line.split() for line in classification_report_str.split('\\n')[2:-5]], headers='firstrow', tablefmt='grid'))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_losses, label='Test Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_labels, all_predictions, 'bo', markersize=3)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('True vs Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the trained model on the eval dataset\n",
    "# test_results = mainTrainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# epoch_accuracies = []\n",
    "# epoch_test_loss = []\n",
    "\n",
    "# for epoch in range(mainTrainer.args.num_train_epochs):\n",
    "#     test_accuracy = test_results['eval_accuracy']\n",
    "#     test_loss = test_results['eval_loss']\n",
    "#     epoch_test_loss.append(test_loss)\n",
    "#     epoch_accuracies.append(test_accuracy)\n",
    "#     print(f\"Epoch {epoch + 1} - Test Accuracy: {test_accuracy}\")\n",
    "#     print(f\"Epoch {epoch + 1} - Test Loss: {test_loss}\")\n",
    "\n",
    "# # Plot accuracy per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_accuracies, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy per Epoch')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot loss per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_test_loss, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss per Epoch')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bWzoJ-yYHsD"
   },
   "source": [
    "# Save the Model\n",
    "We serialize the model for checkpointing and for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.579917Z",
     "start_time": "2024-01-04T22:33:19.576933Z"
    },
    "id": "yQvYHhG1YJ3D"
   },
   "outputs": [],
   "source": [
    "#Save Directory\n",
    "save_directory = \"saved_model\"\n",
    "\n",
    "# Save the trained model\n",
    "mainTrainer.save_model(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkLV3hsYYKm"
   },
   "source": [
    "# Predicting using the base model\n",
    "Utilizing the base model, we predict the label of an image and produce up to five responses with their corresponding relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.584350Z",
     "start_time": "2024-01-04T22:33:19.579586Z"
    },
    "id": "y7ra0pG7YXZq"
   },
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def predict(path, top_k):\n",
    "    # read the image using openCV\n",
    "    image = cv2.imread(path)\n",
    "    # applying the thresholding function for preprocessing\n",
    "    bg1 = remove_background(image)\n",
    "    thresh1 = thresholding(bg1)\n",
    "    image = thresh1\n",
    "    # openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    # resize the image\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    image /= 255.0 \n",
    "    \n",
    "    # This is to show the image after preprocessing. Saved this for debugging.\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(image)\n",
    "    \n",
    "    # ViTFeatureExtractor is deprecated (still work but will give warning). For transformer of version 5+, AutoImageProcessor is used.\n",
    "    # load the model. Should be replaced with our own model later\n",
    "    # model_directory = our_model_dic\n",
    "    # feature_extractor = AutoImageProcessor.from_pretrained(model_directory)\n",
    "    # model = ViTForImageClassification.from_pretrained(model_directory, return_dict=False)\n",
    "     \n",
    "    feature_extractor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # get the top five predictions\n",
    "    top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "\n",
    "    # pack everything in a list \n",
    "    top_k_predictions = [{\"class_idx\": idx.item(), \"score\": score.item()} for idx, score in zip(top_k_indices[0], top_k_values[0])]\n",
    "    for item in top_k_predictions:\n",
    "        item[\"class_label\"] = model.config.id2label[item[\"class_idx\"]]\n",
    "        \n",
    "    for item in top_k_predictions:\n",
    "        del(item[\"class_idx\"])\n",
    "    \n",
    "    return top_k_predictions \n",
    "   \n",
    "\n",
    "top_k_predictions = predict(path, 5)\n",
    "\n",
    "# print the five top predictions and the score they have\n",
    "for prediction in top_k_predictions:\n",
    "    score = prediction[\"score\"]\n",
    "    class_label = prediction[\"class_label\"]\n",
    "    print(f\"Predicted Class: {class_label}, Score: {score}\")\n",
    "\n",
    "# check the whole list\n",
    "print()\n",
    "print(top_k_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using the saved model\n",
    "Utilizing the trained model, we predict the label of an image and produce up to five responses with their corresponding relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def local_predict(path, k):\n",
    "    # First initialized the customized model and the classes that will be used in preprocessing:\n",
    "    # Check if CUDA is available and set the device accordingly\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    class ViTForImageClassification(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "            # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "        def forward(self, pixel_values, labels):\n",
    "            outputs = self.vit(pixel_values=pixel_values)\n",
    "            logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "    \n",
    "\n",
    "    # compute accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        if isinstance(labels, int):\n",
    "            labels = [labels]\n",
    "        accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "    # create feature extractor to tokenize data\n",
    "    feature_extractor = ViTImageProcessor(\n",
    "        image_size=224,\n",
    "        do_resize=True,\n",
    "        do_normalize=True,\n",
    "        do_rescale=False,\n",
    "        image_mean=[0.5, 0.5, 0.5],\n",
    "        image_std=[0.5, 0.5, 0.5],\n",
    "    )\n",
    "\n",
    "\n",
    "    # define a custom data collator\n",
    "    def data_collator(features):\n",
    "        pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "        labels = [feature['labels'] for feature in features]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "    # num_classes = labels.max() + 1\n",
    "    num_classes = 2113\n",
    "\n",
    "\n",
    "    # Define the features of the dataset\n",
    "    features = Features({\n",
    "        'labels': ClassLabel(num_classes=num_classes),\n",
    "        'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "\n",
    "    train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    # Define your custom model\n",
    "    config = pretrained_model.config\n",
    "    config.num_labels = 2112\n",
    "    model = ViTForImageClassification(config)\n",
    "\n",
    "    # Copy the pre-trained weights to your custom model\n",
    "    model.vit = pretrained_model\n",
    "\n",
    "    model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "        early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    "    )\n",
    "\n",
    "\n",
    "    # create the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=50,              # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "        warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.018,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        logging_strategy='steps',\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=10,  \n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        learning_rate=3e-5,\n",
    "        gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "        max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "        # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    "    )\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def get_train_dataloader(self):\n",
    "            return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            # Move inputs to device\n",
    "            for key, value in inputs.items():\n",
    "                inputs[key] = value.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    mainTrainer = CustomTrainer (\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    "    )\n",
    "\n",
    "#     print('done')\n",
    "    \n",
    "    # Second preprocess the input:\n",
    "    # OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "    def load_and_preprocess_user_input(example):\n",
    "#         thresholding(image) not used for now cause it will make the prediction worse      \n",
    "        image = Image.open(path)\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        image = np.moveaxis(image, source=-1, destination=0)\n",
    "        inputs = feature_extractor(images=[image])\n",
    "        pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "        label = example['labels']\n",
    "        return {'pixel_values': pixel_values, 'labels': label}\n",
    "    \n",
    "    # Create the pandas DataFrame using user input. The lable is a random number, won't be used so can be any value.\n",
    "    user_data = [[path, 987]]\n",
    " \n",
    "    # Create the user input pandas DataFrame\n",
    "    user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "    image_paths = user_df['image_paths'].values\n",
    "    labels = user_df[\"labels\"].values\n",
    "    \n",
    "    # Transfer the user input into the object that can be accepted by the model\n",
    "    user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "    user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "    user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "    user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "    user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "    user_test_dataset = user_test_dataset.map(load_and_preprocess_user_input)\n",
    "    user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "    \n",
    "    # Third use the model to predict:\n",
    "    outputs = mainTrainer.predict(user_test_dataset)\n",
    "    \n",
    "    # Fourth use the saved encoder to decode the predictions\n",
    "    y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "    \n",
    "    # Fifth return the list containing top k possibilities\n",
    "    result={}\n",
    "    i = 0\n",
    "    while i < k:\n",
    "        result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "#         print('Rank '+ str(i+1) + ' possibility of the pill: ' + encoder.inverse_transform([y_pred[0][i]])[0])\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "results = local_predict(path, 5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "# def load_and_preprocess_images(example):\n",
    "#     # Load the image from the file\n",
    "#     image = Image.open('dataset/' + example['image_paths'])\n",
    "#     image = np.array(image, dtype=np.uint8)\n",
    "#     image = np.moveaxis(image, source=-1, destination=0)\n",
    "#     # Preprocess the image\n",
    "#     inputs = feature_extractor(images=[image])\n",
    "#     pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "#     label = int(example['labels'])\n",
    "#     return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "# num_classes = labels.max() + 1\n",
    "num_classes = 2113\n",
    "\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "# # Generate lists of image paths and labels for training dataset\n",
    "# train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "# train_labels = train_df[\"labels\"].tolist()\n",
    "\n",
    "# # Create a dictionary with the image paths and labels\n",
    "# train_dict = {'image_paths': train_image_paths, 'labels': train_labels}\n",
    "\n",
    "# # Create the dataset\n",
    "# train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# # Apply the function to the dataset\n",
    "# train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "# train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# # Repeat the same process for the evaluation and test datasets\n",
    "# eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "# eval_labels = eval_df[\"labels\"].tolist()\n",
    "# eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels}\n",
    "# eval_dataset = Dataset.from_dict(eval_dict)\n",
    "# eval_dataset = eval_dataset.map(load_and_preprocess_images)\n",
    "# eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "# test_labels = test_df[\"labels\"].tolist()\n",
    "# test_dict = {'image_paths': test_image_paths, 'labels': test_labels}\n",
    "# test_dataset = Dataset.from_dict(test_dict)\n",
    "# test_dataset = test_dataset.map(load_and_preprocess_images)\n",
    "# test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = 2112\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=10,  \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "def load_and_preprocess_images2(example):\n",
    "#     image = cv2.imread('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    # applying the thresholding function for preprocessing\n",
    "#     image = thresholding(image)\n",
    "    # openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "#     image = tf.image.resize(image, [256, 256])\n",
    "#     image /= 255.0 \n",
    "    image = Image.open('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = example['labels']\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "user_data = [['00002-3228-30_NLMIMAGE10_391E1C80.jpg', 987]]\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "image_paths = user_df['image_paths'].values\n",
    "labels = user_df[\"labels\"].values\n",
    "\n",
    "user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "\n",
    "user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "user_test_dataset = user_test_dataset.map(load_and_preprocess_images2)\n",
    "user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "print(user_test_dataset)\n",
    "\n",
    "# test_dict2 = {'pixel_values': pixel_values, 'labels': 137}\n",
    "\n",
    "# test_dict2 = {'image_paths': '00002-3228-30_NLMIMAGE10_391E1C80.jpg', 'labels': 137}\n",
    "# test_dataset2 = datasets.DatasetDict(test_dict2)\n",
    "# test_dataset2 = Dataset.from_dict(test_dict2)\n",
    "# test_dataset2 = test_dataset2.map(load_and_preprocess_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mainTrainer.predict(user_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = outputs.predictions.argmax(1)\n",
    "y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "print(y_pred[0][4])\n",
    "top_5 = y_pred[0]\n",
    "i = 0\n",
    "result={}\n",
    "while i < 5:\n",
    "    result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "    print('Rank ' + str(i+1) + ' possibility of the pill: ' + str(encoder.inverse_transform([y_pred[0][i]])[0]))\n",
    "    i += 1\n",
    "# label_encoder.inverse_transform([y_pred[0]])[0]\n",
    "# print(outputs.predictions)\n",
    "print(result)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
