{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03mxFGNIXVqF"
   },
   "source": [
    "# Pill Identifier Machine Learning Model and API\n",
    "All pills and tablets have a unique combination of features that allow them to be identified. \n",
    "These features are its color, its shape, and imprints made in front and/or at the back of the drug.\n",
    "\n",
    "This model will utilize [insert model here] for image classification.\n",
    "\n",
    "[more details to be added]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7SczRM2XJyN"
   },
   "source": [
    "# Imports, Declarations, and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:43.358190Z",
     "start_time": "2024-01-04T22:36:38.947503Z"
    },
    "id": "YSVBXMIEXQLa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "# import optuna\n",
    "# import apex                     # for mixed precision training\n",
    "import accelerate\n",
    "import tqdm as notebook_tqdm\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt     # to plot charts\n",
    "import numpy as np\n",
    "import numba as nb                  # to optimize any mathematics\n",
    "import pickle as pkl                # to save our model\n",
    "import sklearn                      # for dataset splitting and metrics\n",
    "import pandas as pd                 # for data manipulation\n",
    "import cv2                          # for image processing\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate       # to print pretty tables\n",
    "\n",
    "# sklearn imports for metrics and dataset splitting\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras imports for image preprocessing\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# huggingface imports for model building \n",
    "import torch.nn as nn\n",
    "from transformers import ViTFeatureExtractor, ViTModel, ViTForImageClassification, TrainingArguments, AutoModel, Trainer, \\\n",
    "  default_data_collator, EarlyStoppingCallback, AutoModelForSequenceClassification, ViTConfig, TrainerState, AutoImageProcessor, ViTImageProcessor \n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# keras imports for early stoppage and model checkpointing\n",
    "from keras.callbacks import ModelCheckpoint   # may no longer be needed\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau  # may no longer be needed\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from datasets import load_dataset, load_metric, Features, ClassLabel, Array3D, Dataset\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"dataset\"\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeJN4BQ3dF0H"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:45.362505Z",
     "start_time": "2024-01-04T22:36:45.356006Z"
    },
    "id": "WC-oRwKNdOqu"
   },
   "outputs": [],
   "source": [
    "# Function to convert an image file to a tensor\n",
    "def image_to_tensor(image_file):\n",
    "    image = Image.open(image_file)\n",
    "    image = Resize((224, 224))(image)\n",
    "    return ToTensor()(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54DLyRiiXkpH"
   },
   "source": [
    "# Data Acquisition\n",
    "Retrieves the images from our dataset and stores them in memory.\n",
    "Corresponding labels are retrieved, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:46.590066Z",
     "start_time": "2024-01-04T22:36:46.524940Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg9m_GC-zWMu",
    "outputId": "9cb4cc80-cc39-4af9-8cde-1902baa34c7d"
   },
   "outputs": [],
   "source": [
    "# read the csv file with labels\n",
    "csv_file = \"table.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_label\"] = label_encoder.fit_transform(df[\"name\"])\n",
    "\n",
    "# create a dataset from the dataframe\n",
    "image_paths = df[\"rxnavImageFileName\"].values\n",
    "labels = df[\"encoded_label\"].values\n",
    "num_labels = len(df[\"encoded_label\"].unique())\n",
    "print(\"Number of labels:\", num_labels)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_df = pd.DataFrame(list(dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    "\n",
    "# print the first 5 image paths and decoded labels\n",
    "for image, label in dataset.take(5):\n",
    "  print(\"Image:\", image.numpy())\n",
    "  print(\"Label:\", label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  print()\n",
    "\n",
    "# np.save('encoder/encoder.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:47.545335Z",
     "start_time": "2024-01-04T22:36:47.513945Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_images(path, label):\n",
    "  image = tf.io.read_file(directory + '/' + path)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.resize(image, [256, 256])\n",
    "  image /= 255.0 \n",
    "  return image, label\n",
    "\n",
    "dataset = dataset.map(load_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-04T22:36:48.374189Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# display the first 9 images and their labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(dataset.take(9)):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image)\n",
    "  plt.title(label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nm5cEhnXQrY"
   },
   "source": [
    "# Data Augmentation and Preprocessing\n",
    "Because each pill/tablet only has one picture, the data set in itself is not ideal.\n",
    "To improve the quality of the data set, and that of the model, we augment the data.\n",
    "We do this by transforming the image, mimicking how an actual user may take a picture.\n",
    "That is, the image can be brightened, resized, rotated, sheared, cropped, and etc. Other processes are also performed to improve training of the model such as splitting the data into a training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxGoMZM6Xpo9",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#Convert column into strings\n",
    "dataset_df[\"image_paths\"] = dataset_df[\"image_paths\"].astype(str)\n",
    "dataset_df[\"labels\"] = dataset_df[\"labels\"].astype(str)\n",
    "\n",
    "print(dataset_df[\"image_paths\"].head())\n",
    "\n",
    "#Splitting dataset into 60/20/20\n",
    "train_df, temp_df = train_test_split(dataset_df, test_size=0.4, random_state=42)\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=1337)\n",
    "\n",
    "# Create the image data generator for the training set\n",
    "imageTrain_data = ImageDataGenerator(\n",
    "    rescale = 1./255.,\n",
    "    rotation_range = 60,\n",
    "    shear_range = 0.3,\n",
    "    zoom_range = 0.5,\n",
    "    vertical_flip = True,\n",
    "    horizontal_flip = True,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "# Create the image data generator for the evaluation set\n",
    "imageEval_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "eval_generator = imageEval_data.flow_from_dataframe(\n",
    "    dataframe=eval_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the image data generator for the test set\n",
    "imageTest_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "test_generator = imageTest_data.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "#Display example of image augmentation\n",
    "sample_dataframe = train_df.sample(n=1).reset_index(drop=True)\n",
    "sample_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=sample_dataframe,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range (0, 15):\n",
    "  ax = plt.subplot(5, 3, i + 1)\n",
    "  for X_column, Y_column in sample_generator:\n",
    "    plt.imshow(X_column[0])\n",
    "    break\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-ZGIIlGXqWY"
   },
   "source": [
    "# Filtering\n",
    "Using OpenCV, we filter out any artifacts (i.e. background, lens flares, graininess, etc.) and extract the features necessary for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X28VD5zqX9Pu",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# apply thresholding to a color image\n",
    "def thresholding(img):\n",
    "  # Split the image into the B,G,R components\n",
    "  b, g, r = cv2.split(img)\n",
    "\n",
    "  # Apply thresholding to each channel\n",
    "  _, b = cv2.threshold(b, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "  _, g = cv2.threshold(g, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "  _, r = cv2.threshold(r, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "  # Merge the channels\n",
    "  thresholded = cv2.merge([b, g, r])\n",
    "\n",
    "  return thresholded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zW9RMHnXyhu"
   },
   "source": [
    "# Hyperparameter Search\n",
    "To ensure the best set of hyperparameters used by the model, we enable hyperparameter search prior to training the model. This exhaustively searches the best combination of hyperparameters to be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the dataframe into a dataset\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = read_image(image_path)  # Open the image and convert it to a tensor\n",
    "        image = Resize((224, 224), antialias=True)(image)  # Resize the image\n",
    "        return {'pixel_values': image, 'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XICqDptZYCI2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "token = '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'             # Jan's personal access token\n",
    "configuration = ViTConfig()\n",
    "\n",
    "# Select only 100 rows from the training set\n",
    "train_df = train_df.sample(n=100)\n",
    "\n",
    "# Prepend the path to the dataset folder to each file path\n",
    "train_df['image_paths'] = train_df['image_paths'].apply(lambda x: x if x.startswith('dataset') else os.path.join('dataset', x))\n",
    "\n",
    "# Split data into a training set and an evaluation set\n",
    "train_df, eval_df = train_test_split(train_df, test_size=0.2)  # Use 20% of your data for evaluation\n",
    "\n",
    "# Reset the index of the DataFrame to avoid indexing errors\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Convert your images and labels to tensors\n",
    "pixel_values = [image_to_tensor(image_file) for image_file in train_df['image_paths']]\n",
    "labels = train_df['labels'].to_numpy()\n",
    "\n",
    "# Create a dictionary with the pixel values and labels\n",
    "train_data = {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = ImageClassificationDataset(train_df['image_paths'], train_df['labels'].to_numpy())\n",
    "eval_dataset = ImageClassificationDataset(eval_df['image_paths'], eval_df['labels'].to_numpy())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    evaluation_strategy=\"steps\",    \n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    ")\n",
    "\n",
    "\n",
    "def model_init(trial):\n",
    "    num_labels = len(np.unique(train_df['labels'].to_numpy()))\n",
    "    configuration.num_labels = num_labels           # Set the number of output units to match the number of classes\n",
    "    return ViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=configuration,\n",
    "        from_tf=bool(\".ckpt\" in model_name),\n",
    "        cache_dir=model_name,                       # use cache to speed up model loading\n",
    "        token=token,\n",
    "        ignore_mismatched_sizes=True                # ignore image size mismatch errors\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "#Execute hyperparameter search\n",
    "hypersearch = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=8, \n",
    ")\n",
    "\n",
    "\n",
    "print(hypersearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning with Dataset Chunks\n",
    " Explore the efficiency of incremental learning with 3.3TB dataset chunks. Optimize training by iteratively downloading, processing, and removing chunks for improved resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "['PillProjectDisc69/images/CLLLLUPGIX7J8MP1WWQ9WN4-CO0B5NV.CR2', 'PillProjectDisc98/images/PRNJ-AXZIQ!HUQKJJBP_DV44ST0KN9.CR2', 'PillProjectDisc10/images/79U-YY6M1UUR6F127ZMACIWPEEXHLB.JPG', 'PillProjectDisc11/images/7WVFV5H74!ELFNQ_GUH92E9ERM9P2K.JPG', 'PillProjectDisc20/images/B4CH0R9B7PEQ6GORRX-8XWOL-_G7W9_.JPG', 'PillProjectDisc21/images/B5T5HI5XI8X2HSBJL-TGDHET4YG5C5F.JPG', 'PillProjectDisc25/images/B8S621VZTSXR4Z4VRHYLTKGCQWIXETR.JPG', 'PillProjectDisc28/images/BBLLJ1DZIAXWHPZA07ZCZ5!YZPLWVBA.JPG', 'PillProjectDisc41/images/BNK95TSS7NYS0O7R0D3EKFZP2GTJUC1.WMV', 'PillProjectDisc44/images/BR0!XO0KYHN2QNKB-R0OSVWFQF2G7X!.JPG', 'PillProjectDisc46/images/BSSE1RTL_WV-P1VJTH2PFT6-RR03UAF.JPG', 'PillProjectDisc66/images/CIGT5HVSPCB8CLSJIC4INJ097O-2!Q0.JPG', 'PillProjectDisc6/images/35KGT-3C1H7WX3A2Q4!31-4L2TLID.JPG', 'PillProjectDisc6/images/3FF0SCXQ626WA86PGXROYJ5G5YLP7H.JPG', 'PillProjectDisc76/images/CRZ-PHXR3OH97ZB9IV6__ROF1US6MSN.JPG', 'PillProjectDisc76/images/CS4YFI_L_-D9SKLKC!6XPP9W901CELH.JPG', 'PillProjectDisc107/images/XOYMJP0D912S321KPLK8D4MKZAAZBN.PNG', 'PillProjectDisc1/images/!XO4!2HPTFD7LRKK7BEITXEXP4H50Q.PNG', 'PillProjectDisc32/images/BFUTE7Y_K_CQK43Y0_VO93PFOOHG7EJ.PNG', 'PillProjectDisc67/images/CJBGFH-3-71!BMQW92T31YU4UBFO817.PNG', 'PillProjectDisc67/images/CJJOP4ZF4YFOC2X8N75YTKBZF7RI_R7.PNG', 'PillProjectDisc88/images/GKM8T_A-NPC6IVNGWT84-U0_IKT3_A.PNG', 'PillProjectDisc94/images/LT_WST26QD!R65Q5O9H_CX13XYQHTT.PNG', 'PillProjectDisc96/images/NQCAM9Q4T8H9TBSIDP8PNDV-F2W28N.PNG', 'PillProjectDisc37/images/BKMNRTQ6_7JVE3M5_DIU81YSSX!P!HB.JPG', 'PillProjectDisc52/images/BYDKBIA7YG6M92Z7WNOG9UC9H_4VFSH.JPG', 'PillProjectDisc106/images/X2L8CKXHOEI90EPDF!23BUS_ML1MVM.PNG', 'PillProjectDisc28/images/BBNN_1BONHSJUULJSL42PBZ8!QJJ0MW.PNG', 'PillProjectDisc56/images/C1EZMMI9VNZ9TBSH11YGQGMD6AZAWIG.PNG', 'PillProjectDisc69/images/CLWJ2!OXX!1N33UW6QCHE7IP90T60KG.PNG', 'PillProjectDisc14/images/B!OXWD76YK16DGPDFRX8XX2WVEDA5TH.CR2', 'PillProjectDisc79/images/CUFUTUYPLAWR7KAF1069EIJH31VORQ!.CR2', 'PillProjectDisc109/images/ZE6YLGQVM5S1CK6YI_DLIDWGHZ8F8W.JPG', 'PillProjectDisc10/images/6I3-4B_PW66465QR7ANNTSZN7M0NIH.JPG', 'PillProjectDisc15/images/B-FYXX8OS1YRV94N!KEU3DIY595NY55.JPG', 'PillProjectDisc16/images/B0QZ_Z_IQWHX4RYF81U87HJ6AKFMPOG.JPG', 'PillProjectDisc22/images/B5WMYTNZQ79IIW4BJGLQ6ZC1VZRV-SR.JPG', 'PillProjectDisc26/images/BA1-IOS7ANYJCE5NYAE5RL_XJVMKHG1.JPG', 'PillProjectDisc28/images/BC17BXCDR9B8930M7UH5A83FJ5LX_ZI.JPG', 'PillProjectDisc35/images/BIV05ACX4JMU3XO6160NJ1P_1PSUX7D.JPG', 'PillProjectDisc43/images/BPUX9Q9TUHE7JWSG-0799AJKSI3_75S.JPG', 'PillProjectDisc45/images/BRL3QG!YPENPS1_75-16UYPVUDXVDT-.JPG', 'PillProjectDisc45/images/BR_YPSX2J5E3K-8_C-PZ52MN3GYS3YH.JPG', 'PillProjectDisc46/images/BSDPRB6AA1-1RF0LE7Y5GD_R4OXFPF_.JPG', 'PillProjectDisc56/images/C0XGMU4AFBNYGKF!UH8QVA3N8MN63UD.JPG', 'PillProjectDisc59/images/CB_Y2300O9TEWY0L6B4SQBX!5VGSM74.JPG', 'PillProjectDisc64/images/CGERHPG3LRLR0X26M43L99OKNDYR-YL.JPG', 'PillProjectDisc67/images/CJE3US-DFBMYWY3F7OM9-7VK9CCO2CQ.JPG', 'PillProjectDisc70/images/CMTAVC7AH84O8ZM1DLGR6TX9V8HEKDV.JPG', 'PillProjectDisc74/images/CPZY_IXL-F6!55KJGT-N7Q!2-DDL9D!.JPG', 'PillProjectDisc74/images/CQF062JBLEY642KHIT_ADFT_XCGMM!I.JPG', 'PillProjectDisc74/images/CQ!JTXN2ALP8SNHTI0BX35XM4_S6Z!Q.JPG', 'PillProjectDisc78/images/CTZ9Z5TT1QVEG21XD98K2X0Z8X9BY2F.JPG', 'PillProjectDisc7/images/3ZZ19PGAED!WRX05HQQNO7BJFCID-!.JPG', 'PillProjectDisc92/images/JP2JW2-9!S84G41Y0QIDQ50I9PZ111.JPG', 'PillProjectDisc92/images/K6QQET!08476TI3778Y4EJ3ILMZ!CR.JPG', 'PillProjectDisc93/images/L--51ECVK9OPH74ZGHF9S2RA_I7NJX.WMV', 'PillProjectDisc98/images/PGW!K872SS_IKY0BRE988YNFEQ3LYG.JPG', 'PillProjectDisc4/images/11AD4K-MOAK7DRFFRF-AR_X!N4WUP6.JPG', 'PillProjectDisc29/images/BCL_K794CQND6K98LUMHBKY_CTZO!G.JPG', 'PillProjectDisc104/images/URUT8NGM3N_Y76P4OXFDNMMWA13YO2.PNG', 'PillProjectDisc109/images/ZLG2P2LAMS4!MRFO6Q87U2SMDRS8IY.PNG', 'PillProjectDisc26/images/B9NBMWEX06D4HV83B!TA706URSV72A-.PNG', 'PillProjectDisc26/images/B9XIODPYGWDR1ZB6SC25LM7M39O8QN2.PNG']\n",
      "\n",
      "Batch 2:\n",
      "['PillProjectDisc47/images/BTR548EHCRBYR97TCA638QYF6FWQ_JF.PNG', 'PillProjectDisc90/images/HZU88V7J5Z!AS4CZ2C-RUF7PM6_Z4B.PNG', 'PillProjectDisc91/images/J5AA2IL87HPUDQPJLR1-JSLM8_FSVC.PNG', 'PillProjectDisc97/images/OL4ZED_59QXGAVRF!3DAM_0WMK6S24.PNG', 'PillProjectDisc43/images/BPJHG_K6IRN91BK5G444NFL92U9_AG5.JPG', 'PillProjectDisc91/images/J372MCFJ9M8ZC2_ZRFJUSVJWR20PYY.JPG', 'PillProjectDisc10/images/6MGD!1!EP_Y9ES3_LIII_2A3VX5A4-.PNG', 'PillProjectDisc76/images/CSMUS!JNQ7RV38_4!R-US!0MNUYK448.PNG', 'PillProjectDisc80/images/CVHIZ045AOO7I00OWB3H6809HW8F3P2.PNG', 'PillProjectDisc98/images/P6HOM66-IQ0-I-L62LL_JG!5UUHCU7.PNG', 'PillProjectDisc11/images/7RP8X7BEHFTJZTEPB74AQC4FX1KLSO.JPG', 'PillProjectDisc17/images/B1IGG6SBC!WXHB7PK0ZRB3M-KCIJF5Y.CR2', 'PillProjectDisc68/images/CKLJXN!_R5YFAUB1R0KGD65CJDT0KR3.CR2', 'PillProjectDisc106/images/WLHJ4ZX9QJLI-RUCMPUI5UGWASU22A.JPG', 'PillProjectDisc106/images/WPU-81TFJF0LIMJJ2UW96XA9UHRKC6.JPG', 'PillProjectDisc109/images/ZTMGV9WE8UBKDP70XYQ!3P_BY8YM4Q.WMV', 'PillProjectDisc110/images/_IFSJ8P2C5OZ7ZK363I5CXOPX-CXC-.JPG', 'PillProjectDisc16/images/B01H7J2X8MK0PP-6V16FINBQVP9H5FQ.JPG', 'PillProjectDisc20/images/B3W8Y0QK631ZZNY1S84U4!UC74OO1OZ.JPG', 'PillProjectDisc20/images/B4RC!VKCGAZU6Y2H-BEO18Y8_YMSA5U.JPG', 'PillProjectDisc26/images/B9U!DYW6J7HIG8D5-NPFO60_JDAHQIR.JPG', 'PillProjectDisc29/images/BCRLVS3BD2D6DX-M8YPOZ8I38BAECJU.JPG', 'PillProjectDisc2/images/-BIC9!843SKI9AG0_1N141_286UXDP.JPG', 'PillProjectDisc35/images/BINZI-PDPZ36VT4NFBE_5CNNF6N3676.JPG', 'PillProjectDisc40/images/BN93L9206R!I20EPY8!VFX-KR55MBPS.JPG', 'PillProjectDisc53/images/BZQ7I_GKV9GMRPSKEK9F!CDHCDRTG_0.JPG', 'PillProjectDisc56/images/C1Z3F0NUH7980G2W!I39_V5N6SG!-PP.JPG', 'PillProjectDisc78/images/CTUESJ5W_WU0_Q706TP674YFVMC55PU.JPG', 'PillProjectDisc97/images/O8G05FI-113UE0P7UN8JXY32PJ96_T.JPG', 'PillProjectDisc52/images/BYCQ3ZS5__B13QXQHTTX1!Q7K4D2IQQ.JPG', 'PillProjectDisc22/images/B6FQYB82CWSC2RUXBNZ_K!_KDEZLQE2.JPG', 'PillProjectDisc101/images/S5DF4HDO8QWW7NHIG2OFBMJ5A8JBUE.PNG', 'PillProjectDisc14/images/B!ZG99F0SARBP4_8GR3U8B6SZQRW_7V.PNG', 'PillProjectDisc15/images/B-O8HAWM2OMXQ1G4FPQIEA02T!MR4K2.PNG', 'PillProjectDisc22/images/B6EJCRE4RXSO672ID3B67TJRV55TE5-.PNG', 'PillProjectDisc35/images/BIMS-SX093WOVOK4DEG64SNKAKDP7C.PNG', 'PillProjectDisc66/images/CI9VS1UJ!9UROZ5SS7!!TD_LWLTFZC_.PNG', 'PillProjectDisc74/images/CQVMYX3LAYMA65V749NYNX5P_44S3UW.PNG', 'PillProjectDisc87/images/EY!UW8!G4YHUO0!R_Y5I-0-B9UG!-B.PNG', 'PillProjectDisc106/images/W9WJ5DVPKN8-DARNMB53SKVF8PU!2N.JPG', 'PillProjectDisc82/images/CXBHH8IVHSL4Q904G3U!BYRK9SU5S5R.JPG', 'PillProjectDisc13/images/B!1DXQO5HMAAE2_0P6D48L_Y61JS0B9.PNG', 'PillProjectDisc83/images/CYBHOXBY646JHCYJ!WU9WD-D5JU9W7Q.PNG', 'PillProjectDisc83/images/CYFO018M5XUEP03-CPO55ISCAEDQ831.PNG', 'PillProjectDisc98/images/PPJ_!IIY6SSQ8FWSDA2O16WMTTI06P.PNG', 'PillProjectDisc21/images/B5H0BERYJXXQAUW13_6MVTS53DGSI4S.JPG', 'PillProjectDisc93/images/KAWGCA124XUIK0IUZYZ5IA6L3AEQJH.CR2', 'PillProjectDisc9/images/5LNKW9O!31KUKUS!E-JSZJXPRWDL!W.CR2', 'PillProjectDisc103/images/U7URT1A5H58YFJMA_G4HS!A7_N0SNY.JPG', 'PillProjectDisc105/images/V_VEEX0ZMIBR-2GB!OKSIGXE3E!-SE.JPG', 'PillProjectDisc106/images/WGK1UE6H7T-EARPNYO4LB09!T_VF9_.JPG', 'PillProjectDisc110/images/_ARQ_E987P-IFV9T!PGX2_6YHQ5IZX.JPG', 'PillProjectDisc17/images/B1SPQX455U47RY4-7YLE0P-AG73-9V3.WMV', 'PillProjectDisc30/images/BDBXV-CX!ESJZD_7P8_HZ1IV!223GVC.JPG', 'PillProjectDisc32/images/BFXYDR13C-_VKBLTNMO6A28P0AU9S07.JPG', 'PillProjectDisc37/images/BK3PPVLDL7ENIE7PUY5BL6FL8MMC4OY.JPG', 'PillProjectDisc51/images/BXY5L_AOWR0X4HI2JT8K9RNVC2NXSD7.JPG', 'PillProjectDisc56/images/C1RCV6PN2!VHTS35QEW_9E0F!TG00P6.JPG', 'PillProjectDisc5/images/2!L1I47YDFNP_ZSVNWN6W3-!0NWK80.JPG', 'PillProjectDisc69/images/CLL_QZPJSK-TIEKZMXWP0!DL1ZGGNS-.JPG', 'PillProjectDisc77/images/CT5NZDS9X0JIGFAWZO0626_YJ17VM!_.JPG', 'PillProjectDisc80/images/CW6GRIQ4Z9J-NFUQ9NRY7F!2RCNXZPR.JPG', 'PillProjectDisc89/images/H1!2VIFPQIDEULYOV5MAC5N97SRZ3I.JPG', 'PillProjectDisc92/images/K06K5G23YPW0T806WDPFG9IHYU1QF5.JPG']\n",
      "\n",
      "Batch 3:\n",
      "['PillProjectDisc12/images/8Y1!TN3F4ONO0EMJ9XE!8-YLS_6LON.JPG', 'PillProjectDisc53/images/BZECRZ8_UHB4XYMGM82-MTIV7KA935H.JPG', 'PillProjectDisc11/images/88T!C0J-62E_!D1DQCBOE2D_0ADKID.PNG', 'PillProjectDisc30/images/BD79TO394PDYDH2Z!7NN_ZT7MXMP8G4.PNG', 'PillProjectDisc33/images/BGFE-A01QCLJDTQEUWR9XP942C-5J6E.PNG', 'PillProjectDisc45/images/BRSLA7I0S8QB6Y_ZVV2NTL1OMF2TS3G.PNG', 'PillProjectDisc4/images/0VRHSK7D0U-MCOJNP17O5XGWN3L1NP.PNG', 'PillProjectDisc51/images/BXFWKZ6EET9JQT5KRIP-_3ORKJQX5A6.PNG', 'PillProjectDisc59/images/CC5_1M-KTUTD3WCFZE0WY6YG2T0PL7!.PNG', 'PillProjectDisc97/images/N_VACZ6IE32GWSKTB9TZAD_KSVVVYG.PNG', 'PillProjectDisc12/images/8LL0A9QTPOWC-E77KNDMUSYD6EQH-S.JPG', 'PillProjectDisc48/images/BUE7-UX95YLAIHJ4!CAFBCWMIJR5JGD.JPG', 'PillProjectDisc55/images/C0A2!HDND1WVC_35C2E7KU-K87BGRAH.PNG', 'PillProjectDisc81/images/CWYA1J1RH-MQ_X!PUOC!N7UX7Z5Z_5Z.PNG', 'PillProjectDisc87/images/FPXF4NJTC_IP7EP52-D1MITK4VEKOS.PNG', 'PillProjectDisc92/images/K!CP1!4Y1ASILUM55X8EK!AA25VKF8.PNG', 'PillProjectDisc61/images/CDY4BN6-M!8ECQ7KK9QL1TA_CN-OHKR.JPG', 'PillProjectDisc26/images/B9_KZ-5JHKC!ACBWDTPB-3PA35K4JZX.CR2', 'PillProjectDisc93/images/KY8H_ZB91AY1J8Y_V6TLUO8SZE4WT4.CR2', 'PillProjectDisc10/images/6MYHFWXWAB96ZP5_MRXNHX-0P_3JO0.JPG', 'PillProjectDisc23/images/B6TGP-K_6_AGE0LY5AOY581-UT715T3.JPG', 'PillProjectDisc2/images/-8BLK8-53HAEI0QNEI2L_UQKAXEJ_7.JPG', 'PillProjectDisc2/images/-CYJI2!27!DZ9MOE83-5K-54DDSKRY.JPG', 'PillProjectDisc41/images/BNWIS3VL!VRL!_5F9PJL17TUF3IFPLU.JPG', 'PillProjectDisc47/images/BT9-0-K6JRJ9E6-SDC2JB-SNICCZZ8G.JPG', 'PillProjectDisc53/images/BZ5-8OIRQZK!!SM!WM2Z3M1M3CXUPJ0.JPG', 'PillProjectDisc65/images/CI!8A2!KIV7LHVEGYDNCB38OUW4HIA8.JPG', 'PillProjectDisc66/images/CILC4L6OREFAEQA7UL0HZCDCLFWKZEX.JPG', 'PillProjectDisc68/images/CKQDB5V0_4FXLEIJOR_MO7ZNN5M!1K5.JPG', 'PillProjectDisc76/images/CRX129ZI8QJTHRVKWNC0ZJ72335OPXD.JPG', 'PillProjectDisc78/images/CT_XXTRWHXVUD8OJA73FZOU2!707LBV.JPG', 'PillProjectDisc81/images/CWO8THXJC1AU46_B!6D0KYGPG8W0FI6.WMV', 'PillProjectDisc81/images/CWTC-VDRSINZLKFQA828WBHYE!C7HPG.JPG', 'PillProjectDisc85/images/CZYTFHR1AR8Z0RF17CROQIIUM-GCMTQ.JPG', 'PillProjectDisc92/images/JE8U__43_4PGL30!ZUY8NFSCS9DGF6.JPG', 'PillProjectDisc79/images/CUN!8BC158Q1!!NZ1ALRNV0FT033T3G.JPG', 'PillProjectDisc19/images/B3J-LY-SD4MVNDWI!H_CEM5GGEIITGR.JPG', 'PillProjectDisc102/images/SU52Q-O-CTLU_LHSG5ZD86T2T!GYZ5.PNG', 'PillProjectDisc16/images/B0N0V_X5SMK!BT89!PBT1WCZK0!XTDO.PNG', 'PillProjectDisc19/images/B30V6YUMM3J3CT1X!SLG2AQ-UAD33D4.PNG', 'PillProjectDisc41/images/BO5Y9UVY1LP!O1QKCBXFKX9164HKQG5.PNG', 'PillProjectDisc43/images/BP_5GCI54QJEA___N6CCIZGAX9P-DNE.PNG', 'PillProjectDisc4/images/0_F-A2WK6H5!VH4LQTRH4YRQ2DY-N8.PNG', 'PillProjectDisc81/images/CX!M_43AS0MFUJIF9DV200IK9XAWCEY.PNG', 'PillProjectDisc87/images/FH_6PYYYKS1OBY!S_VIHXX-QD6B088.PNG', 'PillProjectDisc27/images/BAW7MVDATQZJSPKO-DZI0H3-17GX85O.JPG', 'PillProjectDisc28/images/BBLN8M_AV-O_GYX7YULLWGW1-AHS2H6.JPG', 'PillProjectDisc13/images/9I7KTIRHMIKWPXAGNP74!11RC9J!WQ.PNG', 'PillProjectDisc33/images/BG5JDR2CA_6A0EQEIA8KTM-4S49153X.PNG', 'PillProjectDisc43/images/BPNNZD75ACCZ6EBPVFG5F94XHE_OB7F.PNG', 'PillProjectDisc52/images/BYZKYCUQ2CTE3GBE3VNGDNHBO2ZR-QP.PNG', 'PillProjectDisc89/images/HKKE5I69MVY4MOKPCRAC4E0HA0Z0WE.JPG', 'PillProjectDisc15/images/B-DIYCYWNF6DAODB2PALD23MM1KLAC5.CR2', 'PillProjectDisc97/images/NXCJOCVIQNJKNZQCWB53ZJBBH_IS1W.CR2', 'PillProjectDisc11/images/7_5U7ANLKKODV9IEFUMZR4R9RUQI2_.JPG', 'PillProjectDisc19/images/B2YP7PV2V8UNZJ0F_JHV6LSQAPI3TVE.JPG', 'PillProjectDisc24/images/B7YN3XKLG5C!J1KJHFCB30UHA!WCFFH.WMV', 'PillProjectDisc36/images/BJ7ICW9MJ3886LWBUVSUEK1VXKX-UVA.JPG', 'PillProjectDisc40/images/BNH0_FQ-T4G9AL_H45VENPU7LK-!GH_.JPG', 'PillProjectDisc41/images/BOAK4AHLUQXA29W5IXY1LJJYKCAPISA.JPG', 'PillProjectDisc41/images/BO!AKL7CJCXT2IRI2LBWQH4UJMMPC-8.JPG', 'PillProjectDisc48/images/BV-VZAAOVGF0MQAZ5A-WR2LEL7TP3GW.JPG', 'PillProjectDisc52/images/BY-IUU_27_Z8GMAFWRRFRATQ6M7CAFJ.JPG', 'PillProjectDisc53/images/BZL1KBLQGQ632A9I!VSEDPVMMWIF4TX.JPG']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "DATASET_URL = 'https://data.lhncbc.nlm.nih.gov/public/Pills/'\n",
    "\n",
    "# read the xlsx file\n",
    "xlsx_file = \"./directory_consumer_grade_images.xlsx\"\n",
    "df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "# Iterate over the third column and append to the list\n",
    "image_paths = []\n",
    "for image_path in df.iloc[:, 2]:\n",
    "  image_paths.append(image_path)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Separate image paths into batches\n",
    "image_batches = [image_paths[i:i+batch_size] for i in range(0, len(image_paths), batch_size)]\n",
    "\n",
    "# Now, image_batches is a list of batches, where each batch contains 64 image paths\n",
    "# Let's take a look at the first 3 batches and display it nice\n",
    "for i, batch in enumerate(image_batches[:3]):\n",
    "  print(f\"Batch {i+1}:\")\n",
    "  print(batch)\n",
    "  print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzwPHUrRYCdM"
   },
   "source": [
    "# Model Training\n",
    "We train the model using the best hyperparameters on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "def load_and_preprocess_images(example):\n",
    "    # Load the image from the file\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = int(example['labels'])\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "\n",
    "num_classes = labels.max() + 1\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "# Generate lists of image paths and labels for training dataset\n",
    "train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "train_labels = train_df[\"labels\"].tolist()\n",
    "\n",
    "# Create a dictionary with the image paths and labels\n",
    "train_dict = {'image_paths': train_image_paths, 'labels': train_labels}\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# Repeat the same process for the evaluation and test datasets\n",
    "eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "eval_labels = eval_df[\"labels\"].tolist()\n",
    "eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels}\n",
    "eval_dataset = Dataset.from_dict(eval_dict)\n",
    "eval_dataset = eval_dataset.map(load_and_preprocess_images)\n",
    "eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "test_labels = test_df[\"labels\"].tolist()\n",
    "test_dict = {'image_paths': test_image_paths, 'labels': test_labels}\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_images)\n",
    "test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = num_labels\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=10,  \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    ")\n",
    "\n",
    "# mainTrainer.train()\n",
    "# model.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FQOqnPYEg1"
   },
   "source": [
    "# Model Testing\n",
    "We test the model on the test set to validate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess the images for testing\n",
    "def load_and_preprocess_test_images(example):\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)\n",
    "    label = int(example['labels'])\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_test_images)\n",
    "test_dataset = test_dataset.remove_columns(['image_paths'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_losses = []\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to predictions\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "        # Append predictions and true labels to lists\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_losses.append(loss)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_losses = np.array(all_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "classification_report_str = classification_report(all_labels, all_predictions)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {fscore}\")\n",
    "print(\"Classification Report:\\n\", tabulate([[''] + classification_report_str.split('\\n')[0].split()] + [line.split() for line in classification_report_str.split('\\n')[2:-5]], headers='firstrow', tablefmt='grid'))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_losses, label='Test Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_labels, all_predictions, 'bo', markersize=3)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('True vs Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the trained model on the eval dataset\n",
    "# test_results = mainTrainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# epoch_accuracies = []\n",
    "# epoch_test_loss = []\n",
    "\n",
    "# for epoch in range(mainTrainer.args.num_train_epochs):\n",
    "#     test_accuracy = test_results['eval_accuracy']\n",
    "#     test_loss = test_results['eval_loss']\n",
    "#     epoch_test_loss.append(test_loss)\n",
    "#     epoch_accuracies.append(test_accuracy)\n",
    "#     print(f\"Epoch {epoch + 1} - Test Accuracy: {test_accuracy}\")\n",
    "#     print(f\"Epoch {epoch + 1} - Test Loss: {test_loss}\")\n",
    "\n",
    "# # Plot accuracy per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_accuracies, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy per Epoch')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot loss per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_test_loss, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss per Epoch')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bWzoJ-yYHsD"
   },
   "source": [
    "# Save the Model\n",
    "We serialize the model for checkpointing and for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.579917Z",
     "start_time": "2024-01-04T22:33:19.576933Z"
    },
    "id": "yQvYHhG1YJ3D"
   },
   "outputs": [],
   "source": [
    "#Save Directory\n",
    "save_directory = \"saved_model\"\n",
    "\n",
    "# Save the trained model\n",
    "mainTrainer.save_model(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkLV3hsYYKm"
   },
   "source": [
    "# Predicting using the base model\n",
    "Utilizing the model, we predict the label of an image and produce up to five responses with their corresponding relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.584350Z",
     "start_time": "2024-01-04T22:33:19.579586Z"
    },
    "id": "y7ra0pG7YXZq"
   },
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def predict(path, top_k):\n",
    "    # read the image using openCV\n",
    "    image = cv2.imread(path)\n",
    "    # applying the thresholding function for preprocessing\n",
    "    image = thresholding(image)\n",
    "    # openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    # resize the image\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    image /= 255.0 \n",
    "    \n",
    "    # This is to show the image after preprocessing. Saved this for debugging.\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(image)\n",
    "    \n",
    "    # ViTFeatureExtractor is deprecated (still work but will give warning). For transformer of version 5+, AutoImageProcessor is used.\n",
    "    # load the model. Should be replaced with our own model later\n",
    "    # model_directory = “our_model_dic”\n",
    "    # feature_extractor = AutoImageProcessor.from_pretrained(model_directory)\n",
    "    # model = ViTForImageClassification.from_pretrained(model_directory, return_dict=False)\n",
    "     \n",
    "    feature_extractor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # get the top five predictions\n",
    "    top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "\n",
    "    # pack everything in a list \n",
    "    top_k_predictions = [{\"class_idx\": idx.item(), \"score\": score.item()} for idx, score in zip(top_k_indices[0], top_k_values[0])]\n",
    "    for item in top_k_predictions:\n",
    "        item[\"class_label\"] = model.config.id2label[item[\"class_idx\"]]\n",
    "        \n",
    "    for item in top_k_predictions:\n",
    "        del(item[\"class_idx\"])\n",
    "    \n",
    "    return top_k_predictions \n",
    "   \n",
    "\n",
    "top_k_predictions = predict(path, 5)\n",
    "\n",
    "# print the five top predictions and the score they have\n",
    "for prediction in top_k_predictions:\n",
    "    score = prediction[\"score\"]\n",
    "    class_label = prediction[\"class_label\"]\n",
    "    print(f\"Predicted Class: {class_label}, Score: {score}\")\n",
    "\n",
    "# check the whole list\n",
    "print()\n",
    "print(top_k_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def local_predict(path, k):\n",
    "    # First initialized the customized model and the classes that will be used in preprocessing:\n",
    "    # Check if CUDA is available and set the device accordingly\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    class ViTForImageClassification(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "            # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "        def forward(self, pixel_values, labels):\n",
    "            outputs = self.vit(pixel_values=pixel_values)\n",
    "            logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "    \n",
    "\n",
    "    # compute accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        if isinstance(labels, int):\n",
    "            labels = [labels]\n",
    "        accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "    # create feature extractor to tokenize data\n",
    "    feature_extractor = ViTImageProcessor(\n",
    "        image_size=224,\n",
    "        do_resize=True,\n",
    "        do_normalize=True,\n",
    "        do_rescale=False,\n",
    "        image_mean=[0.5, 0.5, 0.5],\n",
    "        image_std=[0.5, 0.5, 0.5],\n",
    "    )\n",
    "\n",
    "\n",
    "    # define a custom data collator\n",
    "    def data_collator(features):\n",
    "        pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "        labels = [feature['labels'] for feature in features]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "    # num_classes = labels.max() + 1\n",
    "    num_classes = 2113\n",
    "\n",
    "\n",
    "    # Define the features of the dataset\n",
    "    features = Features({\n",
    "        'labels': ClassLabel(num_classes=num_classes),\n",
    "        'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "\n",
    "    train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    # Define your custom model\n",
    "    config = pretrained_model.config\n",
    "    config.num_labels = 2112\n",
    "    model = ViTForImageClassification(config)\n",
    "\n",
    "    # Copy the pre-trained weights to your custom model\n",
    "    model.vit = pretrained_model\n",
    "\n",
    "    model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "        early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    "    )\n",
    "\n",
    "\n",
    "    # create the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=50,              # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "        warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.018,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        logging_strategy='steps',\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=10,  \n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        learning_rate=3e-5,\n",
    "        gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "        max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "        # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    "    )\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def get_train_dataloader(self):\n",
    "            return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            # Move inputs to device\n",
    "            for key, value in inputs.items():\n",
    "                inputs[key] = value.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    mainTrainer = CustomTrainer (\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    "    )\n",
    "\n",
    "#     print('done')\n",
    "    \n",
    "    # Second preprocess the input:\n",
    "    # OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "    def load_and_preprocess_user_input(example):\n",
    "#         thresholding(image) not used for now cause it will make the prediction worse      \n",
    "        image = Image.open(path)\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        image = np.moveaxis(image, source=-1, destination=0)\n",
    "        inputs = feature_extractor(images=[image])\n",
    "        pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "        label = example['labels']\n",
    "        return {'pixel_values': pixel_values, 'labels': label}\n",
    "    \n",
    "    # Create the pandas DataFrame using user input. The lable is a random number, won't be used so can be any value.\n",
    "    user_data = [[path, 987]]\n",
    " \n",
    "    # Create the user input pandas DataFrame\n",
    "    user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "    image_paths = user_df['image_paths'].values\n",
    "    labels = user_df[\"labels\"].values\n",
    "    \n",
    "    # Transfer the user input into the object that can be accepted by the model\n",
    "    user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "    user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "    user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "    user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "    user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "    user_test_dataset = user_test_dataset.map(load_and_preprocess_user_input)\n",
    "    user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "    \n",
    "    # Third use the model to predict:\n",
    "    outputs = mainTrainer.predict(user_test_dataset)\n",
    "    \n",
    "    # Fourth use the saved encoder to decode the predictions\n",
    "    y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "    \n",
    "    # Fifth return the list containing top k possibilities\n",
    "    result={}\n",
    "    i = 0\n",
    "    while i < k:\n",
    "        result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "#         print('Rank '+ str(i+1) + ' possibility of the pill: ' + encoder.inverse_transform([y_pred[0][i]])[0])\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "results = local_predict(path, 5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "# def load_and_preprocess_images(example):\n",
    "#     # Load the image from the file\n",
    "#     image = Image.open('dataset/' + example['image_paths'])\n",
    "#     image = np.array(image, dtype=np.uint8)\n",
    "#     image = np.moveaxis(image, source=-1, destination=0)\n",
    "#     # Preprocess the image\n",
    "#     inputs = feature_extractor(images=[image])\n",
    "#     pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "#     label = int(example['labels'])\n",
    "#     return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "# num_classes = labels.max() + 1\n",
    "num_classes = 2113\n",
    "\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "# # Generate lists of image paths and labels for training dataset\n",
    "# train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "# train_labels = train_df[\"labels\"].tolist()\n",
    "\n",
    "# # Create a dictionary with the image paths and labels\n",
    "# train_dict = {'image_paths': train_image_paths, 'labels': train_labels}\n",
    "\n",
    "# # Create the dataset\n",
    "# train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# # Apply the function to the dataset\n",
    "# train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "# train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# # Repeat the same process for the evaluation and test datasets\n",
    "# eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "# eval_labels = eval_df[\"labels\"].tolist()\n",
    "# eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels}\n",
    "# eval_dataset = Dataset.from_dict(eval_dict)\n",
    "# eval_dataset = eval_dataset.map(load_and_preprocess_images)\n",
    "# eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "# test_labels = test_df[\"labels\"].tolist()\n",
    "# test_dict = {'image_paths': test_image_paths, 'labels': test_labels}\n",
    "# test_dataset = Dataset.from_dict(test_dict)\n",
    "# test_dataset = test_dataset.map(load_and_preprocess_images)\n",
    "# test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = 2112\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=10,  \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "def load_and_preprocess_images2(example):\n",
    "#     image = cv2.imread('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    # applying the thresholding function for preprocessing\n",
    "#     image = thresholding(image)\n",
    "    # openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "#     image = tf.image.resize(image, [256, 256])\n",
    "#     image /= 255.0 \n",
    "    image = Image.open('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = example['labels']\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "user_data = [['00002-3228-30_NLMIMAGE10_391E1C80.jpg', 987]]\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "image_paths = user_df['image_paths'].values\n",
    "labels = user_df[\"labels\"].values\n",
    "\n",
    "user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "\n",
    "user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "user_test_dataset = user_test_dataset.map(load_and_preprocess_images2)\n",
    "user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "print(user_test_dataset)\n",
    "\n",
    "# test_dict2 = {'pixel_values': pixel_values, 'labels': 137}\n",
    "\n",
    "# test_dict2 = {'image_paths': '00002-3228-30_NLMIMAGE10_391E1C80.jpg', 'labels': 137}\n",
    "# test_dataset2 = datasets.DatasetDict(test_dict2)\n",
    "# test_dataset2 = Dataset.from_dict(test_dict2)\n",
    "# test_dataset2 = test_dataset2.map(load_and_preprocess_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mainTrainer.predict(user_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = outputs.predictions.argmax(1)\n",
    "y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "print(y_pred[0][4])\n",
    "top_5 = y_pred[0]\n",
    "i = 0\n",
    "result={}\n",
    "while i < 5:\n",
    "    result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "    print('Rank ' + str(i+1) + ' possibility of the pill: ' + str(encoder.inverse_transform([y_pred[0][i]])[0]))\n",
    "    i += 1\n",
    "# label_encoder.inverse_transform([y_pred[0]])[0]\n",
    "# print(outputs.predictions)\n",
    "print(result)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
