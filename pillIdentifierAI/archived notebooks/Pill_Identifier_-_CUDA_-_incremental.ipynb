{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03mxFGNIXVqF"
   },
   "source": [
    "# Pill Identifier Machine Learning Model and API\n",
    "All pills and tablets have a unique combination of features that allow them to be identified. These features are its color, its shape, and imprints made in front and/or at the back of the drug. The model aims to predict the name of an unknown pill/tablet based on these features.\n",
    "\n",
    "This model will utilize google/vit-base-patch16-224 for image classification.\n",
    "\n",
    "The dataset is from the U.S. Department of Health's Computational Photography Project for Pill Identification (C3PI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7SczRM2XJyN"
   },
   "source": [
    "# Imports, Declarations, and Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:43.358190Z",
     "start_time": "2024-01-04T22:36:38.947503Z"
    },
    "id": "YSVBXMIEXQLa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jong\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import image as mpimg\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt     # to plot charts\n",
    "import numpy as np\n",
    "import pandas as pd                 # for data manipulation\n",
    "import cv2                          # for image processing\n",
    "from io import BytesIO\n",
    "from tabulate import tabulate       # to print pretty tables\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn imports for metrics and dataset splitting\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras imports for image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# huggingface imports for model building \n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTForImageClassification, TrainingArguments, Trainer, \\\n",
    "  default_data_collator, EarlyStoppingCallback, ViTConfig, AutoImageProcessor, ViTImageProcessor \n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# keras imports for early stoppage and model checkpointing\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from datasets import load_dataset, load_metric, Features, ClassLabel, Array3D, Dataset\n",
    "import datasets\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from safetensors import safe_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://data.lhncbc.nlm.nih.gov/public/Pills/'\n",
    "directory = \"dataset\"\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeJN4BQ3dF0H"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:45.362505Z",
     "start_time": "2024-01-04T22:36:45.356006Z"
    },
    "id": "WC-oRwKNdOqu"
   },
   "outputs": [],
   "source": [
    "# Function to convert an image file to a tensor\n",
    "def image_to_tensor(image_file):\n",
    "    image = Image.open(image_file)\n",
    "    image = Resize((224, 224))(image)\n",
    "    return ToTensor()(image)\n",
    "\n",
    "#calculate the weights\n",
    "def get_weight(class_num, label_count):\n",
    "    weights = 1 / np.log(label_count)\n",
    "    weights = class_num * weights/np.sum(weights)\n",
    "    return weights\n",
    "\n",
    "def add_class_weights(input_data):\n",
    "    #get the number of labels\n",
    "    result_data = input_data\n",
    "    label_num = len(result_data['labels'].unique())\n",
    "    \n",
    "    #Create a Pandas dataframe for weight caculation\n",
    "    value = result_data.value_counts('labels').tolist()\n",
    "    value_df = pd.DataFrame({'labels': result_data.value_counts('labels').index.tolist(), 'counts':result_data.value_counts('labels').tolist()})\n",
    "    \n",
    "    base = 2\n",
    "    value_df['counts'] = get_weight(label_num, base*value_df['counts'])\n",
    "    # value_df\n",
    "    list = value_df.set_index('labels').T.to_dict('list')\n",
    "    \n",
    "    for index, row in result_data.iterrows():\n",
    "        result_data.loc[index, ('weights')] = list[result_data.loc[index, ('labels')]][0]\n",
    "    \n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_ruler(image_path):\n",
    "    # Read the image using PIL\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # If the image has an alpha channel, convert it to RGB\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "\n",
    "    # Get image dimensions\n",
    "    width, height = img.size\n",
    "\n",
    "    # Define default crop amounts\n",
    "    left_crop = 25\n",
    "    bottom_crop = 20\n",
    "    right_crop = 0  \n",
    "    top_crop = 0 \n",
    "\n",
    "    if width > 225 and height < 260:\n",
    "        left_crop = 8\n",
    "        bottom_crop = 18\n",
    "    if width > 225 and height > 260:\n",
    "        left_crop = 8\n",
    "        bottom_crop = 57\n",
    "    if width > 1000:\n",
    "        left_crop = 50\n",
    "        bottom_crop = 300\n",
    "    if width >= 1920:\n",
    "        left_crop = 100\n",
    "        bottom_crop = 400\n",
    "        right_crop = 100\n",
    "    if width > 3000:\n",
    "        left_crop = 1000\n",
    "        bottom_crop = 1200\n",
    "        right_crop = 2000 \n",
    "        top_crop = 300\n",
    "\n",
    "\n",
    "    # Crop a portion from the left\n",
    "    img_cropped_left = img.crop((left_crop, top_crop, width, height))\n",
    "\n",
    "    # Crop a portion from the right\n",
    "    img_cropped_right = img_cropped_left.crop((0, 0, width - right_crop, height))\n",
    "\n",
    "    # Crop a portion from the top\n",
    "    img_cropped_top = img_cropped_right.crop((0, top_crop, width - right_crop, height))\n",
    "\n",
    "    # Crop a portion from the bottom\n",
    "    img_cropped_bottom = img_cropped_top.crop((0, 0, width - right_crop, height - bottom_crop))        \n",
    "\n",
    "    # Resize the cropped image back to the original size\n",
    "    img_resized = img_cropped_bottom.resize((img_height, img_width))\n",
    "    # img_resized = img_cropped_bottom.resize((width, height))\n",
    "\n",
    "    zoom_factor = 2\n",
    "\n",
    "    # Calculate the new size for zooming out\n",
    "    new_width = int((width - left_crop - right_crop) / zoom_factor)\n",
    "    new_height = int((height - top_crop - bottom_crop) / zoom_factor)\n",
    "\n",
    "    # Resize the cropped image to achieve a zoom-out effect\n",
    "    img_zoomed_out = img_resized.resize((new_width, new_height))\n",
    "\n",
    "    # Convert the resized image back to a numpy array\n",
    "    resized_array = np.array(img_resized) / 255.0\n",
    "\n",
    "    return resized_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54DLyRiiXkpH"
   },
   "source": [
    "# Data Acquisition\n",
    "Retrieves the images from our dataset and stores them in memory.\n",
    "Corresponding labels are retrieved, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:46.590066Z",
     "start_time": "2024-01-04T22:36:46.524940Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zg9m_GC-zWMu",
    "outputId": "9cb4cc80-cc39-4af9-8cde-1902baa34c7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 2112\n",
      "Image: b'00093-0311-01_NLMIMAGE10_6315B1FD.jpg'\n",
      "Label: Loperamide Hydrochloride 2 MG Oral Capsule\n",
      "\n",
      "Image: b'00093-3165-01_NLMIMAGE10_19270CA8.jpg'\n",
      "Label: Minocycline 50 MG Oral Capsule\n",
      "\n",
      "Image: b'00093-0810-01_NLMIMAGE10_34271A58.jpg'\n",
      "Label: Nortriptyline 10 MG Oral Capsule\n",
      "\n",
      "Image: b'00093-0811-01_NLMIMAGE10_15270A98.jpg'\n",
      "Label: Nortriptyline 25 MG Oral Capsule\n",
      "\n",
      "Image: b'00093-0812-01_NLMIMAGE10_DD0E6EE3.jpg'\n",
      "Label: Nortriptyline 50 MG Oral Capsule\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read the csv file with labels\n",
    "csv_file = \"table.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"encoded_label\"] = label_encoder.fit_transform(df[\"name\"])\n",
    "\n",
    "# create a dataset from the dataframe\n",
    "image_paths = df[\"nlmImageFileName\"].values\n",
    "# image_paths = df[\"rxnavImageFileName\"].values\n",
    "labels = df[\"encoded_label\"].values\n",
    "num_labels = len(df[\"encoded_label\"].unique())\n",
    "print(\"Number of labels:\", num_labels)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_df = pd.DataFrame(list(dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    "\n",
    "# print the first 5 image paths and decoded labels\n",
    "for image, label in dataset.take(5):\n",
    "  print(\"Image:\", image.numpy())\n",
    "  print(\"Label:\", label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  print()\n",
    "\n",
    "# np.save('encoder/encoder.npy', label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:36:47.545335Z",
     "start_time": "2024-01-04T22:36:47.513945Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_images(path, label):\n",
    "  image = tf.io.read_file(directory + '/' + path)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.resize(image, [256, 256])\n",
    "  image /= 255.0\n",
    "  return image, label\n",
    "\n",
    "dataset = dataset.map(load_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-04T22:36:48.374189Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: dataset/PillProjectDisc69/images/CLLLLUPGIX7J8MP1WWQ9WN4-CO0B5NV.CR2 : The system cannot find the path specified.\r\n; No such process\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# display the first 9 images and their labels\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43max\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubplot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:810\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:773\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 773\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    778\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3029\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3027\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3028\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3029\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3030\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3031\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py:5883\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   5881\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[0;32m   5882\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m-> 5883\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NewRandomAccessFile failed to Create/Open: dataset/PillProjectDisc69/images/CLLLLUPGIX7J8MP1WWQ9WN4-CO0B5NV.CR2 : The system cannot find the path specified.\r\n; No such process\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the first 9 images and their labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(dataset.take(9)):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image)\n",
    "  plt.title(label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add datapoints to dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_file = \"directory_consumer_grade_images.xlsx\"\n",
    "df_xls = pd.read_excel(excel_file)\n",
    "\n",
    "# encode the labels\n",
    "df_xls[\"encoded_label\"] = label_encoder.fit_transform(df_xls[\"Name\"])\n",
    "\n",
    "# Check for missing labels\n",
    "missing_labels = df_xls['encoded_label'].isnull().sum()\n",
    "print(\"Number of missing labels:\", missing_labels)\n",
    "\n",
    "# Remove data points without a label\n",
    "df_xls = df_xls.dropna(subset=['encoded_label'])\n",
    "\n",
    "# Check the number of labels again\n",
    "print(\"Total number of labels after removing missing labels: \", len(df_xls[\"encoded_label\"]))\n",
    "\n",
    "# Combine old and new df\n",
    "df_combined = pd.concat([df, df_xls])\n",
    "\n",
    "# Check if the data is imbalanced\n",
    "label_counts = df_combined['encoded_label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Check if the data is imbalanced\n",
    "label_counts = df_combined['encoded_label'].value_counts()\n",
    "\n",
    "# Plot the label counts\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(label_counts, bins=250, alpha=0.5, color='g')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Number of Images per Label')\n",
    "plt.ylabel('Number of Labels')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oversampling and Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined.drop(\"encoded_label\", axis=1)\n",
    "Y = df_combined[\"encoded_label\"]\n",
    "\n",
    "oversampler = RandomOverSampler(sampling_strategy=\"not majority\")\n",
    "X_resampled, Y_resampled = oversampler.fit_resample(X,Y)\n",
    "\n",
    "#Check if dataset is balanced after oversampling\n",
    "print(\"oversampling strategy used on dataset\")\n",
    "print(Y_resampled.value_counts())\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(Y_resampled, bins=250, alpha=0.5, color='g')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label number')\n",
    "plt.ylabel('Number of Images per Label')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "X = df_combined.drop(\"encoded_label\", axis=1)\n",
    "Y = df_combined[\"encoded_label\"]\n",
    "\n",
    "undersampler = RandomUnderSampler(sampling_strategy=\"not minority\")\n",
    "X_resampled, Y_resampled = undersampler.fit_resample(X,Y)\n",
    "\n",
    "#Check if dataset is balanced after undersampling\n",
    "print(\"undersampling strategy used on dataset\")\n",
    "print(Y_resampled.value_counts())\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(Y_resampled, bins=250, alpha=0.5, color='g')\n",
    "plt.title('Distribution of Labels')\n",
    "plt.xlabel('Label number')\n",
    "plt.ylabel('Number of Images per Label')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Nm5cEhnXQrY"
   },
   "source": [
    "# Data Augmentation and Preprocessing\n",
    "Because each pill/tablet only has one picture, the data set in itself is not ideal.\n",
    "To improve the quality of the data set, and that of the model, we augment the data.\n",
    "We do this by transforming the image, mimicking how an actual user may take a picture.\n",
    "That is, the image can be brightened, resized, rotated, sheared, cropped, and etc. Other processes are also performed to improve training of the model such as splitting the data into a training set and testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qxGoMZM6Xpo9",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 78585 invalid image filename(s) in x_col=\"image_paths\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 26195 invalid image filename(s) in x_col=\"image_paths\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 26195 invalid image filename(s) in x_col=\"image_paths\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\janon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\preprocessing\\image.py:1137: UserWarning: Found 1 invalid image filename(s) in x_col=\"image_paths\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m   ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m X_column, Y_column \u001b[38;5;129;01min\u001b[39;00m sample_generator:\n\u001b[1;32m---> 81\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mX_column\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     83\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Convert column into strings\n",
    "dataset_df = add_class_weights(dataset_df)\n",
    "dataset_df[\"image_paths\"] = dataset_df[\"image_paths\"].astype(str)\n",
    "dataset_df[\"labels\"] = dataset_df[\"labels\"].astype(str)\n",
    "\n",
    "# dataset_df = dataset_df.sample(n=500, random_state=42)\n",
    "\n",
    "#Splitting dataset into 60/20/20\n",
    "train_df, temp_df = train_test_split(dataset_df, test_size=0.4, random_state=42)\n",
    "eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=1337)\n",
    "\n",
    "# Create the image data generator for the training set\n",
    "imageTrain_data = ImageDataGenerator(\n",
    "    rescale = 1./255.,\n",
    "    rotation_range = 60,\n",
    "    shear_range = 0.3,\n",
    "    zoom_range = 0.5,\n",
    "    # vertical_flip = True,\n",
    "    # horizontal_flip = True,\n",
    "    width_shift_range = 0.3,\n",
    "    height_shift_range = 0.3,\n",
    "    # brightness_range=[0.5, 1.5],\n",
    "    fill_mode=\"nearest\",\n",
    ")\n",
    "\n",
    "train_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "# Create the image data generator for the evaluation set\n",
    "imageEval_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "eval_generator = imageEval_data.flow_from_dataframe(\n",
    "    dataframe=eval_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create the image data generator for the test set\n",
    "imageTest_data = ImageDataGenerator(rescale = 1./255.)\n",
    "\n",
    "\n",
    "test_generator = imageTest_data.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "#Display example of image augmentation\n",
    "sample_dataframe = train_df.sample(n=1).reset_index(drop=True)\n",
    "sample_generator = imageTrain_data.flow_from_dataframe(\n",
    "    dataframe=sample_dataframe,\n",
    "    directory=directory,\n",
    "    target_size = (img_height, img_width),\n",
    "    batch_size = batch_size,\n",
    "    x_col = \"image_paths\",\n",
    "    y_col = \"labels\",\n",
    "    class_mode=\"categorical\",\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range (0, 15):\n",
    "  ax = plt.subplot(5, 3, i + 1)\n",
    "  for X_column, Y_column in sample_generator:\n",
    "    plt.imshow(X_column[0])\n",
    "    break\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "dataset_df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-ZGIIlGXqWY"
   },
   "source": [
    "# Filtering\n",
    "Using OpenCV, we filter out any artifacts (i.e. background, lens flares, graininess, etc.) and extract the features necessary for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X28VD5zqX9Pu",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "def thresholding(img, alpha=0.5):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply CLAHE to the grayscale image\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(2,2))\n",
    "    gray_clahe = clahe.apply(gray)\n",
    "\n",
    "    # Convert the grayscale image back to BGR\n",
    "    img_clahe = cv2.cvtColor(gray_clahe, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Blend the CLAHE image with the original image\n",
    "    result = cv2.addWeighted(img, alpha, img_clahe, 1 - alpha, 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_background(image):\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a blur\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Threshold the image\n",
    "    _, thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out small contours based on area\n",
    "    contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 500]\n",
    "\n",
    "    # Create an empty mask to store the result\n",
    "    mask = np.zeros_like(thresh)\n",
    "\n",
    "    # Draw the contours on the mask\n",
    "    cv2.drawContours(mask, contours, -1, (255), thickness=cv2.FILLED)\n",
    "\n",
    "    # Perform morphological operations\n",
    "    kernel = np.ones((3,3),np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "    mask = cv2.dilate(mask,kernel,iterations = 1)\n",
    "\n",
    "    # Invert the mask\n",
    "    mask = cv2.bitwise_not(mask)\n",
    "\n",
    "    # Bitwise AND the mask and the original image\n",
    "    res = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4zW9RMHnXyhu"
   },
   "source": [
    "# Hyperparameter Search\n",
    "To ensure the best set of hyperparameters used by the model, we enable hyperparameter search prior to training the model. This exhaustively searches the best combination of hyperparameters to be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the dataframe into a dataset\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = read_image(image_path)  # Open the image and convert it to a tensor\n",
    "        image = Resize((224, 224), antialias=True)(image)  # Resize the image\n",
    "        return {'pixel_values': image, 'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XICqDptZYCI2",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "%\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "token = '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'             # Jan's personal access token\n",
    "configuration = ViTConfig()\n",
    "\n",
    "# Select only 100 rows from the training set\n",
    "train_df = train_df.sample(n=100)\n",
    "\n",
    "# Prepend the path to the dataset folder to each file path\n",
    "train_df['image_paths'] = train_df['image_paths'].apply(lambda x: x if x.startswith('dataset') else os.path.join('dataset', x))\n",
    "\n",
    "# Split data into a training set and an evaluation set\n",
    "train_df, eval_df = train_test_split(train_df, test_size=0.2)  # Use 20% of your data for evaluation\n",
    "\n",
    "# Reset the index of the DataFrame to avoid indexing errors\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "eval_df = eval_df.reset_index(drop=True)\n",
    "\n",
    "# Convert your images and labels to tensors\n",
    "pixel_values = [image_to_tensor(image_file) for image_file in train_df['image_paths']]\n",
    "labels = train_df['labels'].to_numpy()\n",
    "\n",
    "# Create a dictionary with the pixel values and labels\n",
    "train_data = {'pixel_values': pixel_values, 'labels': labels}\n",
    "\n",
    "# Instantiate datasets\n",
    "train_dataset = ImageClassificationDataset(train_df['image_paths'], train_df['labels'].to_numpy())\n",
    "eval_dataset = ImageClassificationDataset(eval_df['image_paths'], eval_df['labels'].to_numpy())\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    evaluation_strategy=\"steps\",    \n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    ")\n",
    "\n",
    "\n",
    "def model_init(trial):\n",
    "    num_labels = len(np.unique(train_df['labels'].to_numpy()))\n",
    "    configuration.num_labels = num_labels           # Set the number of output units to match the number of classes\n",
    "    return ViTForImageClassification.from_pretrained(\n",
    "        model_name,\n",
    "        config=configuration,\n",
    "        from_tf=bool(\".ckpt\" in model_name),\n",
    "        cache_dir=model_name,                       # use cache to speed up model loading\n",
    "        token=token,\n",
    "        ignore_mismatched_sizes=True                # ignore image size mismatch errors\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "\n",
    "#Execute hyperparameter search\n",
    "hypersearch = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=8, \n",
    ")\n",
    "\n",
    "\n",
    "print(hypersearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incremental Learning with Dataset Chunks\n",
    "Explore the efficiency of incremental learning with 3.3TB dataset chunks. Optimize training by iteratively downloading, processing, and removing chunks for improved resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: b'PillProjectDisc37/images/BKMNRTQ6_7JVE3M5_DIU81YSSX!P!HB.JPG'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc52/images/BYDKBIA7YG6M92Z7WNOG9UC9H_4VFSH.JPG'\n",
      "Label: STRATTERA 10MG\n",
      "\n",
      "Image: b'PillProjectDisc43/images/BPJHG_K6IRN91BK5G444NFL92U9_AG5.JPG'\n",
      "Label: STRATTERA CAP 25 MG\n",
      "\n",
      "Image: b'PillProjectDisc91/images/J372MCFJ9M8ZC2_ZRFJUSVJWR20PYY.JPG'\n",
      "Label: STRATTERA CAP 25 MG\n",
      "\n",
      "Image: b'PillProjectDisc106/images/W9WJ5DVPKN8-DARNMB53SKVF8PU!2N.JPG'\n",
      "Label: STRATTERA CAP 40 MG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# read the xlsx file\n",
    "xlsx_file = \"./directory_consumer_grade_images.xlsx\"\n",
    "df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# Filter the DataFrame based on the 'Layout' column\n",
    "# df = df[df['Layout'].isin(['MC_CHALLENGE_V1.0', 'MC_SPL_IMAGE_V3.0', 'C3PI_Test', 'MC_API_RXNAV_V1.3'])]\n",
    "# df = df[df['Layout'].isin(['C3PI_Test'])]\n",
    "df = df[df['Layout'].isin(['MC_CHALLENGE_V1.0'])]\n",
    "\n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "# Iterate over the third column and append to the list\n",
    "image_label_mapping = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    image_path = row.iloc[2]  \n",
    "    label = row.iloc[4]  \n",
    "    if not image_path.endswith('.WMV'):\n",
    "        image_label_mapping[image_path] = label\n",
    "\n",
    "# get the image paths and labels\n",
    "image_paths = list(image_label_mapping.keys())\n",
    "labels = list(image_label_mapping.values())\n",
    "\n",
    "# encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# create a dataset from the dataframe\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset_df = pd.DataFrame(list(dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    "\n",
    "dataset_df = add_class_weights(dataset_df)\n",
    "\n",
    "# print the first 5 image paths and decoded labels\n",
    "for image, label in dataset.take(5):\n",
    "  print(\"Image:\", image.numpy())\n",
    "  print(\"Label:\", label_encoder.inverse_transform([label.numpy()])[0])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets (60/20/20)\n",
    "# train_df, temp_df = train_test_split(dataset_df, test_size=0.4, random_state=42)\n",
    "# eval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "train_df, eval_df = train_test_split(dataset_df, train_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['image_paths'].values, train_df['labels'].values))\n",
    "eval_dataset = tf.data.Dataset.from_tensor_slices((eval_df['image_paths'].values, eval_df['labels'].values))\n",
    "# test_dataset = tf.data.Dataset.from_tensor_slices((test_df['image_paths'].values, test_df['labels'].values))\n",
    "\n",
    "# # Shuffle and batch the datasets\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=len(train_df)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "# test_dataset = test_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# # Load the first batch of data\n",
    "# first_batch = next(iter(train_dataset))\n",
    "\n",
    "# # Unpack the batch\n",
    "# image_paths, labels = first_batch\n",
    "\n",
    "# plt.figure(figsize=(15, 15))\n",
    "# for i in range(len(image_paths)):\n",
    "#     image_path_str = image_paths[i].numpy().decode('utf-8')\n",
    "#     image = Image.open(BytesIO(requests.get(DATASET_URL + image_path_str).content))\n",
    "#     ax = plt.subplot(8, 8, i + 1) \n",
    "#     plt.imshow(image)\n",
    "#     plt.title(label_encoder.inverse_transform([labels[i].numpy()])[0])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzwPHUrRYCdM"
   },
   "source": [
    "# Model Training\n",
    "We train the model using the best hyperparameters on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ae0726be734207a2adcd3500f74c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3526 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23d461f7bcf4f8a85c8129c230262fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/882 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)\n",
    "        self.loss_fct = nn.CrossEntropyLoss(weight=torch.ones(config.num_labels).to(device))\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "def load_and_preprocess_images(example):\n",
    "    # Load the image from the file\n",
    "    # image = Image.open('dataset/' + example['image_paths'])       # loads the image from file\n",
    "    response = requests.get(DATASET_URL + example['image_paths'].decode('utf-8'))       # loads the image from URL\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "\n",
    "    # Convert RGBA images to RGB\n",
    "    if image.mode == 'RGBA':\n",
    "        image = image.convert('RGB')\n",
    "\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    \n",
    "    # Ensure the image has a channel dimension\n",
    "    if image.ndim == 2:\n",
    "        image = image[:, :, np.newaxis]\n",
    "    # Move the channel dimension to the front if necessary\n",
    "    if image.shape[-1] in [1, 3, 4]:\n",
    "        image = np.moveaxis(image, source=-1, destination=0)\n",
    "\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = int(example['labels'])\n",
    "    weights = example['weights']\n",
    "    return {'pixel_values': pixel_values, 'labels': label, 'weights': weights}\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    weights = [feature.get('weights', 1) for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device), 'weights': torch.tensor(weights).to(device)}  # Move to device\n",
    "\n",
    "\n",
    "# num_classes = labels.max() + 1\n",
    "num_classes = tf.reduce_max(labels) + 1\n",
    "\n",
    "\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'weights': Array3D(dtype=\"float32\", shape=(1,)),\n",
    "})\n",
    "\n",
    "# Generate lists of image paths and labels for training dataset\n",
    "train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "train_labels = train_df[\"labels\"].tolist()\n",
    "train_weights = train_df[\"weights\"].tolist()\n",
    "\n",
    "# Create a dictionary with the image paths and labels\n",
    "train_dict = {'image_paths': train_image_paths, 'labels': train_labels, 'weights': train_weights}\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "train_dataset = train_dataset.map(load_and_preprocess_images, remove_columns=None)\n",
    "# train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "# Shuffle, batch, and prefetch the dataset\n",
    "\n",
    "# Repeat the same process for the evaluation and test datasets\n",
    "eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "eval_labels = eval_df[\"labels\"].tolist()\n",
    "eval_weights = eval_df[\"weights\"].tolist()\n",
    "eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels, 'weights': eval_weights}\n",
    "eval_dataset = Dataset.from_dict(eval_dict)\n",
    "eval_dataset = eval_dataset.map(load_and_preprocess_images, remove_columns=None)\n",
    "# eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "# test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "# test_labels = test_df[\"labels\"].tolist()\n",
    "# test_weights = test_df[\"weights\"].tolist()\n",
    "# test_dict = {'image_paths': test_image_paths, 'labels': test_labels, 'weights': test_weights}\n",
    "# test_dataset = Dataset.from_dict(test_dict)\n",
    "# test_dataset = test_dataset.map(load_and_preprocess_images, remove_columns=None)\n",
    "# test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.save(dataset=train_dataset, path=\"./mappings/train_dataset\", shard_func=None, compression=None)\n",
    "tf.data.experimental.save(dataset=eval_dataset, path=\"./mappings/eval_dataset\", shard_func=None, compression=None)\n",
    "tf.data.experimental.save(dataset=test_dataset, path=\"./mappings/test_dataset\", shard_func=None, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image_paths', 'labels', 'weights', 'pixel_values']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.ViTForImageClassification'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the tensors from the .safetensors file\n",
    "tensors = {}\n",
    "with safe_open(\"./saved_model/model.safetensors\", framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "        tensors[k] = f.get_tensor(k)\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = 2112\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Remove unexpected keys from state dictionary\n",
    "tensors = {k: v for k, v in tensors.items() if k not in [\"vit.pooler.dense.bias\", \"vit.pooler.dense.weight\"]}\n",
    "\n",
    "# Load the weights into your custom model\n",
    "model.load_state_dict(tensors)\n",
    "\n",
    "print(type(model))\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=10,              # total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=100,  \n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, model, args, train_dataset, eval_dataset, compute_metrics, data_collator, class_weights, callbacks=None):\n",
    "        super().__init__(model=model, args=args, train_dataset=train_dataset, eval_dataset=eval_dataset, compute_metrics=compute_metrics, data_collator=data_collator, callbacks=callbacks)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        class ImageURLDataset(Dataset):\n",
    "            def __init__(self, data_frame, transform=None):\n",
    "                self.data_frame = data_frame\n",
    "                self.transform = transform\n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.data_frame)\n",
    "\n",
    "            def __getitem__(self, idx):\n",
    "                item = self.data_frame[idx]\n",
    "                images = []\n",
    "                for image_path in item['image_paths']:\n",
    "                    image_path_str = image_path.decode('utf-8')  # decode bytes to string\n",
    "                    image_url = DATASET_URL + image_path_str  # append base URL to image path\n",
    "                    response = requests.get(image_url)  # send GET request to image URL\n",
    "                    image = Image.open(BytesIO(response.content))  # load image from response content\n",
    "                    image = self.transform(image)  # convert image to tensor\n",
    "                    images.append(image)\n",
    "                images = torch.stack(images)  # stack images into a single tensor\n",
    "                label = torch.tensor(item['labels'], dtype=torch.long)  # convert labels to tensor\n",
    "                \n",
    "                return {'pixel_values': images, 'labels': label}\n",
    "\n",
    "        # Define the transformations\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((img_height, img_width)),\n",
    "            transforms.RandomRotation(60),\n",
    "            transforms.RandomResizedCrop((img_height, img_width), scale=(0.5, 1.0)),\n",
    "            transforms.RandomAffine(0, shear=0.3),\n",
    "            transforms.RandomAffine(0, translate=(0.3, 0.3)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "        # Create a DataLoader\n",
    "        dataset = ImageURLDataset(self.train_dataset, transform=train_transform)\n",
    "        return DataLoader(dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        labels = inputs.pop(\"labels\")  # Get labels from inputs\n",
    "        weights = inputs.pop(\"weights\")  # Get weights from inputs\n",
    "\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
    "        losses = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        # Apply sample weights\n",
    "        weighted_losses = losses * weights\n",
    "        loss = weighted_losses.mean()\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Create a weights tensor\n",
    "weights = torch.ones(1656).to(device)  # Creates a tensor of size 2112 with all values set to 1\n",
    "\n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=weights,\n",
    "    # callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.experimental.load(path=\"./mappings/train_dataset\")\n",
    "# tf.data.experimental.load(path=\"./mappings/eval_dataset\")\n",
    "# tf.data.experimental.load(path=\"./mappings/test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596762e791fa4c209f7f945a07a7f493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jong\\AppData\\Local\\Temp\\ipykernel_3008\\2272127800.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9106, 'learning_rate': 3.092783505154639e-06, 'epoch': 9.11}\n",
      "{'loss': 5.8925, 'learning_rate': 2.4742268041237115e-06, 'epoch': 9.29}\n",
      "{'loss': 5.8772, 'learning_rate': 1.8556701030927837e-06, 'epoch': 9.46}\n",
      "{'loss': 5.8774, 'learning_rate': 1.2371134020618557e-06, 'epoch': 9.64}\n",
      "{'loss': 5.9285, 'learning_rate': 6.185567010309279e-07, 'epoch': 9.82}\n",
      "{'loss': 5.9049, 'learning_rate': 0.0, 'epoch': 10.0}\n",
      "{'train_runtime': 3029.4605, 'train_samples_per_second': 11.639, 'train_steps_per_second': 0.185, 'train_loss': 0.6319838387625558, 'epoch': 10.0}\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m mainTrainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/checkpoint-500\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmainTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./saved_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2886\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   2883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   2885\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2886\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2888\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   2889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\trainer.py:2952\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   2950\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer.model is not a `PreTrainedModel`, only saving its state dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors:\n\u001b[1;32m-> 2952\u001b[0m     \u001b[43msafetensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSAFE_WEIGHTS_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m   2954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2955\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2956\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\safetensors\\torch.py:281\u001b[0m, in \u001b[0;36msave_file\u001b[1;34m(tensors, filename, metadata)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_file\u001b[39m(\n\u001b[0;32m    251\u001b[0m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor],\n\u001b[0;32m    252\u001b[0m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    253\u001b[0m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    254\u001b[0m ):\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mSafetensorError\u001b[0m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "mainTrainer.train('./results/checkpoint-500')\n",
    "mainTrainer.save_model('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to add Class weights column to a Pandas Dataframe\n",
    "#### The dataframe must have a column called 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the weights\n",
    "def get_weight(class_num, label_count):\n",
    "    weights = 1 / np.log(label_count)\n",
    "    weights = class_num * weights/np.sum(weights)\n",
    "    return weights\n",
    "\n",
    "def add_class_weights(input_data):\n",
    "    #get the number of labels\n",
    "    result_data = input_data\n",
    "    label_num = len(result_data['labels'].unique())\n",
    "    \n",
    "    #Create a Pandas dataframe for weight caculation\n",
    "    value = result_data.value_counts('labels').tolist()\n",
    "    value_df = pd.DataFrame({'labels': result_data.value_counts('labels').index.tolist(), 'counts':result_data.value_counts('labels').tolist()})\n",
    "    \n",
    "    base = 2\n",
    "    value_df['counts'] = get_weight(label_num, base*value_df['counts'])\n",
    "    # value_df\n",
    "    list = value_df.set_index('labels').T.to_dict('list')\n",
    "    \n",
    "    for index, row in result_data.iterrows():\n",
    "        result_data.loc[index, ('weights')] = list[result_data.loc[index, ('labels')]][0]\n",
    "    \n",
    "    return result_data\n",
    "\n",
    "# test = add_class_weights(dataset_df)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_df = pd.DataFrame({'labels': test['labels'],'weights': test['weights']})\n",
    "\n",
    "frequency_counts = extracted_df['labels'].value_counts().reset_index()\n",
    "frequency_counts.columns = ['labels', 'counts']\n",
    "merged_df = pd.merge(extracted_df, frequency_counts, on='labels', how='right').drop_duplicates('labels')\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.scatterplot(x=merged_df['counts'], y=merged_df['weights'], hue=merged_df['counts'], s=50)\n",
    "plt.xlabel('Frequency of weights')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('Frequency vs Weights')\n",
    "plt.legend(title='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New training methods with class weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "train_labels = train_df[\"labels\"].tolist()\n",
    "train_weights = train_df[\"weights\"].tolist()\n",
    "\n",
    "# Create a dictionary with the image paths and labels\n",
    "train_dict = {'image_paths': train_image_paths, 'labels': train_labels, 'weights': train_weights}\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "train_dataset = train_dataset.remove_columns(['image_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(example):\n",
    "    # Load the image from the file\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = int(example['labels'])\n",
    "    weights = example['weights']\n",
    "    return {'pixel_values': pixel_values, 'labels': label, 'weights': weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'weights': Array3D(dtype=\"float32\", shape=(1,)),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    weights = [feature['weights'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device), 'weights': torch.tensor(weights).to(device)}  # Move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "        weights = inputs[\"weights\"]  # Get weights from inputs\n",
    "        \n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0FQOqnPYEg1"
   },
   "source": [
    "# Model Testing\n",
    "We test the model on the test set to validate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess the images for testing\n",
    "def load_and_preprocess_test_images(example):\n",
    "    image = Image.open('dataset/' + example['image_paths'])\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)\n",
    "    label = int(example['labels'])\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "test_dataset = Dataset.from_dict(test_dict)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_test_images)\n",
    "test_dataset = test_dataset.remove_columns(['image_paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_losses = []\n",
    "\n",
    "# Iterate through the test dataset\n",
    "for batch in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        inputs = batch['pixel_values']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(pixel_values=inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to predictions\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "        # Append predictions and true labels to lists\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_losses.append(loss)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "all_losses = np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "precision, recall, fscore, _ = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
    "classification_report_str = classification_report(all_labels, all_predictions)\n",
    "\n",
    "# Print or use the metrics as needed\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {fscore}\")\n",
    "print(\"Classification Report:\\n\", tabulate([[''] + classification_report_str.split('\\n')[0].split()] + [line.split() for line in classification_report_str.split('\\n')[2:-5]], headers='firstrow', tablefmt='grid'))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(all_losses, label='Test Loss')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(all_labels, all_predictions, 'bo', markersize=3)\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('True vs Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the trained model on the eval dataset\n",
    "# test_results = mainTrainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# epoch_accuracies = []\n",
    "# epoch_test_loss = []\n",
    "\n",
    "# for epoch in range(mainTrainer.args.num_train_epochs):\n",
    "#     test_accuracy = test_results['eval_accuracy']\n",
    "#     test_loss = test_results['eval_loss']\n",
    "#     epoch_test_loss.append(test_loss)\n",
    "#     epoch_accuracies.append(test_accuracy)\n",
    "#     print(f\"Epoch {epoch + 1} - Test Accuracy: {test_accuracy}\")\n",
    "#     print(f\"Epoch {epoch + 1} - Test Loss: {test_loss}\")\n",
    "\n",
    "# # Plot accuracy per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_accuracies, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy per Epoch')\n",
    "# plt.show()\n",
    "\n",
    "# # Plot loss per epoch\n",
    "# plt.plot(range(1, mainTrainer.args.num_train_epochs + 1), epoch_test_loss, marker='o')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss per Epoch')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7bWzoJ-yYHsD"
   },
   "source": [
    "# Save the Model\n",
    "We serialize the model for checkpointing and for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.579917Z",
     "start_time": "2024-01-04T22:33:19.576933Z"
    },
    "id": "yQvYHhG1YJ3D"
   },
   "outputs": [],
   "source": [
    "#Save Directory\n",
    "save_directory = \"saved_model\"\n",
    "\n",
    "# Save the trained model\n",
    "mainTrainer.save_model(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvkLV3hsYYKm"
   },
   "source": [
    "# Predicting using the base model\n",
    "Utilizing the model, we predict the label of an image and produce up to five responses with their corresponding relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-04T22:33:19.584350Z",
     "start_time": "2024-01-04T22:33:19.579586Z"
    },
    "id": "y7ra0pG7YXZq"
   },
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def predict(path, top_k):\n",
    "    # read the image using openCV\n",
    "    image = cv2.imread(path)\n",
    "    # applying the thresholding function for preprocessing\n",
    "    bg1 = remove_background(image)\n",
    "    thresh1 = thresholding(image)\n",
    "    image = thresh1\n",
    "#     openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "    # resize the image\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    image /= 255.0 \n",
    "    \n",
    "    # This is to show the image after preprocessing. Saved this for debugging.\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # plt.imshow(image)\n",
    "    \n",
    "    # ViTFeatureExtractor is deprecated (still work but will give warning). For transformer of version 5+, AutoImageProcessor is used.\n",
    "    # load the model. Should be replaced with our own model later\n",
    "    # model_directory = our_model_dic\n",
    "    # feature_extractor = AutoImageProcessor.from_pretrained(model_directory)\n",
    "    # model = ViTForImageClassification.from_pretrained(model_directory, return_dict=False)\n",
    "     \n",
    "    feature_extractor = AutoImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "    \n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # get the top five predictions\n",
    "    top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
    "\n",
    "    # pack everything in a list \n",
    "    top_k_predictions = [{\"class_idx\": idx.item(), \"score\": score.item()} for idx, score in zip(top_k_indices[0], top_k_values[0])]\n",
    "    for item in top_k_predictions:\n",
    "        item[\"class_label\"] = model.config.id2label[item[\"class_idx\"]]\n",
    "        \n",
    "    for item in top_k_predictions:\n",
    "        del(item[\"class_idx\"])\n",
    "    \n",
    "    return top_k_predictions \n",
    "   \n",
    "\n",
    "top_k_predictions = predict(path, 5)\n",
    "\n",
    "# print the five top predictions and the score they have\n",
    "for prediction in top_k_predictions:\n",
    "    score = prediction[\"score\"]\n",
    "    class_label = prediction[\"class_label\"]\n",
    "    print(f\"Predicted Class: {class_label}, Score: {score}\")\n",
    "\n",
    "# check the whole list\n",
    "print()\n",
    "print(top_k_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting using the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your own path\n",
    "path = \"00002-3228-30_NLMIMAGE10_391E1C80.jpg\"\n",
    "\n",
    "def local_predict(path, k):\n",
    "    # First initialized the customized model and the classes that will be used in preprocessing:\n",
    "    # Check if CUDA is available and set the device accordingly\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    class ViTForImageClassification(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.config = config\n",
    "            # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "            # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "            self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "            self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "        def forward(self, pixel_values, labels):\n",
    "            outputs = self.vit(pixel_values=pixel_values)\n",
    "            logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "            return SequenceClassifierOutput(\n",
    "                loss=loss,\n",
    "                logits=logits,\n",
    "                hidden_states=outputs.hidden_states,\n",
    "                attentions=outputs.attentions,\n",
    "            )\n",
    "    \n",
    "\n",
    "    # compute accuracy\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        if isinstance(labels, int):\n",
    "            labels = [labels]\n",
    "        accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "    # create feature extractor to tokenize data\n",
    "    feature_extractor = ViTImageProcessor(\n",
    "        image_size=224,\n",
    "        do_resize=True,\n",
    "        do_normalize=True,\n",
    "        do_rescale=False,\n",
    "        image_mean=[0.5, 0.5, 0.5],\n",
    "        image_std=[0.5, 0.5, 0.5],\n",
    "    )\n",
    "\n",
    "\n",
    "    # define a custom data collator\n",
    "    def data_collator(features):\n",
    "        pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "        labels = [feature['labels'] for feature in features]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "    # num_classes = labels.max() + 1\n",
    "    num_classes = 2113\n",
    "\n",
    "\n",
    "    # Define the features of the dataset\n",
    "    features = Features({\n",
    "        'labels': ClassLabel(num_classes=num_classes),\n",
    "        'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "        'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    })\n",
    "\n",
    "\n",
    "    train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "    # Define your custom model\n",
    "    config = pretrained_model.config\n",
    "    config.num_labels = 2112\n",
    "    model = ViTForImageClassification(config)\n",
    "\n",
    "    # Copy the pre-trained weights to your custom model\n",
    "    model.vit = pretrained_model\n",
    "\n",
    "    model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "    early_stopping_callback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "        early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    "    )\n",
    "\n",
    "\n",
    "    # create the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=50,              # total number of training epochs\n",
    "        per_device_train_batch_size=16,  # batch size per device during training\n",
    "        per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "        warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.018,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "        logging_steps=10,\n",
    "        logging_first_step=True,\n",
    "        logging_strategy='steps',\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=10,  \n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='accuracy',\n",
    "        greater_is_better=True,\n",
    "        learning_rate=3e-5,\n",
    "        gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "        max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "        # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    "    )\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def get_train_dataloader(self):\n",
    "            return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            # Move inputs to device\n",
    "            for key, value in inputs.items():\n",
    "                inputs[key] = value.to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "            loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    mainTrainer = CustomTrainer (\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=train_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    "    )\n",
    "\n",
    "#     print('done')\n",
    "    \n",
    "    # Second preprocess the input:\n",
    "    # OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "    def load_and_preprocess_user_input(example):\n",
    "#         thresholding(image) not used for now cause it will make the prediction worse      \n",
    "        image = Image.open(path)\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "        image = np.moveaxis(image, source=-1, destination=0)\n",
    "        inputs = feature_extractor(images=[image])\n",
    "        pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "        label = example['labels']\n",
    "        return {'pixel_values': pixel_values, 'labels': label}\n",
    "    \n",
    "    # Create the pandas DataFrame using user input. The lable is a random number, won't be used so can be any value.\n",
    "    user_data = [[path, 987]]\n",
    " \n",
    "    # Create the user input pandas DataFrame\n",
    "    user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "    image_paths = user_df['image_paths'].values\n",
    "    labels = user_df[\"labels\"].values\n",
    "    \n",
    "    # Transfer the user input into the object that can be accepted by the model\n",
    "    user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "    user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "    user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "    user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "    user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "    user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "    user_test_dataset = user_test_dataset.map(load_and_preprocess_user_input)\n",
    "    user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "    \n",
    "    # Third use the model to predict:\n",
    "    outputs = mainTrainer.predict(user_test_dataset)\n",
    "    \n",
    "    # Fourth use the saved encoder to decode the predictions\n",
    "    y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "    \n",
    "    # Fifth return the list containing top k possibilities\n",
    "    result={}\n",
    "    i = 0\n",
    "    while i < k:\n",
    "        result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "#         print('Rank '+ str(i+1) + ' possibility of the pill: ' + encoder.inverse_transform([y_pred[0][i]])[0])\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "results = local_predict(path, 5)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # self.vit = ViTModel(config, add_pooling_layer=False)\n",
    "        # self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.vit = ViTModel(config, add_pooling_layer=False).to(device)                # Move model to Nvidia card\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels).to(device)  # Move model to Nvidia card\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0])\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "\n",
    "# compute accuracy\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    if isinstance(labels, int):\n",
    "        labels = [labels]\n",
    "    accuracy = load_metric(\"accuracy\").compute(predictions=predictions, references=labels)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "    \n",
    "        \n",
    "# create feature extractor to tokenize data\n",
    "feature_extractor = ViTImageProcessor(\n",
    "    image_size=224,\n",
    "    do_resize=True,\n",
    "    do_normalize=True,\n",
    "    do_rescale=False,\n",
    "    image_mean=[0.5, 0.5, 0.5],\n",
    "    image_std=[0.5, 0.5, 0.5],\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess the images\n",
    "# def load_and_preprocess_images(example):\n",
    "#     # Load the image from the file\n",
    "#     image = Image.open('dataset/' + example['image_paths'])\n",
    "#     image = np.array(image, dtype=np.uint8)\n",
    "#     image = np.moveaxis(image, source=-1, destination=0)\n",
    "#     # Preprocess the image\n",
    "#     inputs = feature_extractor(images=[image])\n",
    "#     pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "#     label = int(example['labels'])\n",
    "#     return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# define a custom data collator\n",
    "def data_collator(features):\n",
    "    pixel_values = [torch.tensor(feature['pixel_values'], dtype=torch.float32).to(device) for feature in features]  # Move to device\n",
    "    labels = [feature['labels'] for feature in features]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return {'pixel_values': pixel_values, 'labels': torch.tensor(labels).to(device)}  # Move to device\n",
    "\n",
    "# num_classes = labels.max() + 1\n",
    "num_classes = 2113\n",
    "\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'labels': ClassLabel(num_classes=num_classes),\n",
    "    'img': Array3D(dtype=\"int64\", shape=(3, 32, 32)),\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "})\n",
    "\n",
    "# # Generate lists of image paths and labels for training dataset\n",
    "# train_image_paths = train_df[\"image_paths\"].tolist()\n",
    "# train_labels = train_df[\"labels\"].tolist()\n",
    "\n",
    "# # Create a dictionary with the image paths and labels\n",
    "# train_dict = {'image_paths': train_image_paths, 'labels': train_labels}\n",
    "\n",
    "# # Create the dataset\n",
    "# train_dataset = Dataset.from_dict(train_dict)\n",
    "\n",
    "# # Apply the function to the dataset\n",
    "# train_dataset = train_dataset.map(load_and_preprocess_images)\n",
    "# train_dataset = train_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# # Repeat the same process for the evaluation and test datasets\n",
    "# eval_image_paths = eval_df[\"image_paths\"].tolist()\n",
    "# eval_labels = eval_df[\"labels\"].tolist()\n",
    "# eval_dict = {'image_paths': eval_image_paths, 'labels': eval_labels}\n",
    "# eval_dataset = Dataset.from_dict(eval_dict)\n",
    "# eval_dataset = eval_dataset.map(load_and_preprocess_images)\n",
    "# eval_dataset = eval_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "\n",
    "# test_image_paths = test_df[\"image_paths\"].tolist()\n",
    "# test_labels = test_df[\"labels\"].tolist()\n",
    "# test_dict = {'image_paths': test_image_paths, 'labels': test_labels}\n",
    "# test_dataset = Dataset.from_dict(test_dict)\n",
    "# test_dataset = test_dataset.map(load_and_preprocess_images)\n",
    "# test_dataset = test_dataset.remove_columns(['image_paths'])\n",
    "\n",
    "train_dataset = Dataset.from_dict({'pixel_values': 'pixel_values', 'labels': 'label6789101'})\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "pretrained_model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# Define your custom model\n",
    "config = pretrained_model.config\n",
    "config.num_labels = 2112\n",
    "model = ViTForImageClassification(config)\n",
    "\n",
    "# Copy the pre-trained weights to your custom model\n",
    "model.vit = pretrained_model\n",
    "\n",
    "model.load_state_dict(torch.load('./saved_model/model_weights.pth'))\n",
    "\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,  # Number of evaluations with no improvement after which training will be stopped.\n",
    "    early_stopping_threshold=0.0  # Threshold for measuring the new optimum, to only focus on significant changes.\n",
    ")\n",
    "\n",
    "\n",
    "# create the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    warmup_steps=75,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.018,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=10,  \n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='accuracy',\n",
    "    greater_is_better=True,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,      # prevents vanishing/exploding gradients\n",
    "    max_grad_norm=1.0,                  # prevents vanishing/exploding gradients\n",
    "    # fp16=True                     # mixed precision training; enable if using nVidia graphics cards\n",
    ")\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.args.train_batch_size, shuffle=True, collate_fn=self.data_collator)\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Move inputs to device\n",
    "        for key, value in inputs.items():\n",
    "            inputs[key] = value.to(device)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        labels = inputs[\"labels\"]  # Get labels from inputs\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "mainTrainer = CustomTrainer (\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping_callback]  # Add the early stopping callback\n",
    "\n",
    ")\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV follows BGR convention and PIL follows RGB color convention\n",
    "def load_and_preprocess_images2(example):\n",
    "#     image = cv2.imread('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    # applying the thresholding function for preprocessing\n",
    "#     image = thresholding(image)\n",
    "    # openCV reads image in BGR, convert it to RGB for tensorflow\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n",
    "#     image = tf.image.resize(image, [256, 256])\n",
    "#     image /= 255.0 \n",
    "    image = Image.open('00002-3228-30_NLMIMAGE10_391E1C80.jpg')\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = np.moveaxis(image, source=-1, destination=0)\n",
    "    # Preprocess the image\n",
    "    inputs = feature_extractor(images=[image])\n",
    "    pixel_values = torch.tensor(inputs['pixel_values'][0], dtype=torch.float32).to(device)  # convert to tensor and move to device\n",
    "    label = example['labels']\n",
    "    return {'pixel_values': pixel_values, 'labels': label}\n",
    "\n",
    "\n",
    "# Create the pandas DataFrame\n",
    "user_data = [['00002-3228-30_NLMIMAGE10_391E1C80.jpg', 987]]\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "user_df = pd.DataFrame(user_data, columns=['image_paths', 'labels'])\n",
    "\n",
    "image_paths = user_df['image_paths'].values\n",
    "labels = user_df[\"labels\"].values\n",
    "\n",
    "user_dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "user_dataset_df = pd.DataFrame(list(user_dataset.as_numpy_iterator()), columns=['image_paths', 'labels'])\n",
    " \n",
    "\n",
    "user_test_image_paths = user_dataset_df[\"image_paths\"].tolist()\n",
    "user_test_labels = user_dataset_df[\"labels\"].tolist()\n",
    "user_test_dict = {'image_paths': user_test_image_paths, 'labels': user_test_labels}\n",
    "user_test_dataset = Dataset.from_dict(user_test_dict)\n",
    "user_test_dataset = user_test_dataset.map(load_and_preprocess_images2)\n",
    "user_test_dataset = user_test_dataset.remove_columns(['image_paths'])\n",
    "print(user_test_dataset)\n",
    "\n",
    "# test_dict2 = {'pixel_values': pixel_values, 'labels': 137}\n",
    "\n",
    "# test_dict2 = {'image_paths': '00002-3228-30_NLMIMAGE10_391E1C80.jpg', 'labels': 137}\n",
    "# test_dataset2 = datasets.DatasetDict(test_dict2)\n",
    "# test_dataset2 = Dataset.from_dict(test_dict2)\n",
    "# test_dataset2 = test_dataset2.map(load_and_preprocess_images2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = mainTrainer.predict(user_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = outputs.predictions.argmax(1)\n",
    "y_pred = np.argsort(outputs.predictions, axis=1)[:, ::-1][:, :5]\n",
    "encoder = LabelEncoder()\n",
    "encoder.classes_ = np.load('encoder/encoder.npy', allow_pickle=True)\n",
    "print(y_pred[0][4])\n",
    "top_5 = y_pred[0]\n",
    "i = 0\n",
    "result={}\n",
    "while i < 5:\n",
    "    result[i+1] = encoder.inverse_transform([y_pred[0][i]])[0]\n",
    "    print('Rank ' + str(i+1) + ' possibility of the pill: ' + str(encoder.inverse_transform([y_pred[0][i]])[0]))\n",
    "    i += 1\n",
    "# label_encoder.inverse_transform([y_pred[0]])[0]\n",
    "# print(outputs.predictions)\n",
    "print(result)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
