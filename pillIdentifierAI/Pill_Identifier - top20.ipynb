{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image as mpimg\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import os\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt     # to plot charts\n",
    "import numpy as np\n",
    "import pandas as pd                 # for data manipulation\n",
    "import cv2                          # for image processing\n",
    "from io import BytesIO\n",
    "from tabulate import tabulate       # to print pretty tables\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "\n",
    "# sklearn imports for metrics and dataset splitting\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras imports for image preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# huggingface imports for model building \n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, ViTForImageClassification, TrainingArguments, Trainer, \\\n",
    "  default_data_collator, EarlyStoppingCallback, ViTConfig, AutoImageProcessor, ViTImageProcessor \n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# keras imports for early stoppage and model checkpointing\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from datasets import load_dataset, load_metric, Features, ClassLabel, Array3D, Dataset\n",
    "import datasets\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv files\n",
    "csv_file_top20 = \"./top20.csv\"\n",
    "csv_file = \"./table.csv\"\n",
    "csv_file2 = \"./directory_consumer_grade_images.xlsx\"\n",
    "top20_df = pd.read_csv(csv_file_top20)\n",
    "table_df = pd.read_csv(csv_file)\n",
    "directory_df = pd.read_excel(csv_file2)\n",
    "\n",
    "top20_list = top20_df['Name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to get the base label\n",
    "def get_base_label(label):\n",
    "    for item in top20_list:\n",
    "        if item.lower() in label.lower():\n",
    "            return item\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the top 20 medications in the two datasets\n",
    "# find matches in table_df\n",
    "matches_in_table_df = pd.DataFrame()\n",
    "for item in top20_list:\n",
    "    matches = table_df[table_df['name'].str.contains(item, case=False, na=False) & \n",
    "                       ~table_df['name'].str.contains('and|/', case=False, na=False)]           # remove rows with 'and' or '/' in the name\n",
    "    matches_in_table_df = pd.concat([matches_in_table_df, matches])\n",
    "\n",
    "# find matches in directory_df\n",
    "matches_in_directory_df = pd.DataFrame()\n",
    "for item in top20_list:\n",
    "    matches = directory_df[directory_df['Name'].str.contains(item, case=False, na=False) & \n",
    "                           ~directory_df['Name'].str.contains('and|/', case=False, na=False)]    # remove rows with 'and' or '/' in the name\n",
    "    matches_in_directory_df = pd.concat([matches_in_directory_df, matches])\n",
    "\n",
    "# generate the test set\n",
    "test_df = matches_in_directory_df[matches_in_directory_df['Layout'] == 'C3PI_Test']\n",
    "\n",
    "# keep only necessary images\n",
    "matches_in_directory_df = matches_in_directory_df[matches_in_directory_df['Layout'].isin(['MC_API_NLMIMAGE_V1.3', 'MC_CHALLENGE_V1.0'])]\n",
    "\n",
    "# remove unnecessary columns and rename columns\n",
    "matches_in_table_df = matches_in_table_df[['name', 'nlmImageFileName']]\n",
    "matches_in_table_df = matches_in_table_df.rename(columns={'name': 'labels', 'nlmImageFileName': 'image_paths'})\n",
    "matches_in_directory_df = matches_in_directory_df[['Image', 'Name']]\n",
    "matches_in_directory_df = matches_in_directory_df.rename(columns={'Image': 'image_paths', 'Name': 'labels'})\n",
    "test_df = test_df[['Image', 'Name']]\n",
    "test_df = test_df.rename(columns={'Image': 'image_paths', 'Name': 'labels'})\n",
    "\n",
    "# add a base label column for the top 20 medications\n",
    "matches_in_table_df['base_label'] = matches_in_table_df['labels'].apply(get_base_label)\n",
    "matches_in_directory_df['base_label'] = matches_in_directory_df['labels'].apply(get_base_label)\n",
    "test_df['base_label'] = test_df['labels'].apply(get_base_label)\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "matches_in_table_df['labels'] = encoder.fit_transform(matches_in_table_df['labels'])\n",
    "matches_in_directory_df['labels'] = encoder.fit_transform(matches_in_directory_df['labels'])\n",
    "test_df['labels'] = encoder.fit_transform(test_df['labels'])\n",
    "\n",
    "top20_instances_df = pd.concat([matches_in_table_df, matches_in_directory_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training set size: ',top20_instances_df.size)\n",
    "print('test set size: ',test_df.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_instances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the data is imbalanced in the training set\n",
    "label_counts = top20_instances_df['base_label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# Plot the label counts\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(label_counts.index, label_counts.values, alpha=0.5, color='g')\n",
    "plt.title('Distribution of Base Labels (Training Set)')\n",
    "plt.xlabel('Base Label')\n",
    "plt.ylabel('Number of Labels')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Check if the data is imbalanced in the test set\n",
    "test_label_counts = test_df['base_label'].value_counts()\n",
    "print(test_label_counts)\n",
    "\n",
    "# Plot the label counts for the test set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(test_label_counts.index, test_label_counts.values, alpha=0.5, color='b')\n",
    "plt.title('Distribution of Base Labels (Test Set)')\n",
    "plt.xlabel('Base Label')\n",
    "plt.ylabel('Number of Labels')\n",
    "plt.xticks(rotation=90)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_url = 'https://data.lhncbc.nlm.nih.gov/public/Pills/'\n",
    "dataset_dir = './dataset'\n",
    "training_dir = './training20_set'\n",
    "\n",
    "# Make sure the training directory exists\n",
    "if not os.path.exists(training_dir):\n",
    "    os.makedirs(training_dir)\n",
    "\n",
    "# Function to download an image from a URL and save it to a directory\n",
    "def download_image(url, save_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            response.raw.decode_content = True\n",
    "            shutil.copyfileobj(response.raw, f)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to download image from {url}\")\n",
    "        return False\n",
    "\n",
    "for index, row in top20_instances_df.iterrows():\n",
    "    file_name = row['image_paths']\n",
    "    if os.path.exists(os.path.join(dataset_dir, file_name)):\n",
    "        shutil.copy(os.path.join(dataset_dir, file_name), os.path.join(training_dir, os.path.basename(file_name)))\n",
    "    else:\n",
    "        url = website_url + file_name\n",
    "        save_path = os.path.join(training_dir, os.path.basename(file_name))\n",
    "        if download_image(url, save_path):\n",
    "            print(f\"Downloaded {file_name} from {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to find {file_name} in dataset_dir and download from {url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dir = './testing20_set'\n",
    "\n",
    "# Make sure the testing directory exists\n",
    "if not os.path.exists(testing_dir):\n",
    "    os.makedirs(testing_dir)\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    file_name = row['image_paths']\n",
    "    \n",
    "    # Check if the file ends with \".wmv\", if so, skip it\n",
    "    if file_name.endswith('.WMV'):\n",
    "        print(f\"Skipping {file_name} as it has the .wmv extension\")\n",
    "        continue\n",
    "    \n",
    "    if os.path.exists(os.path.join(dataset_dir, file_name)):\n",
    "        shutil.copy(os.path.join(dataset_dir, file_name), os.path.join(testing_dir, os.path.basename(file_name)))\n",
    "    else:\n",
    "        url = website_url + file_name\n",
    "        save_path = os.path.join(testing_dir, os.path.basename(file_name))\n",
    "        if download_image(url, save_path):\n",
    "            print(f\"Downloaded {file_name} from {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to find {file_name} in dataset_dir and download from {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the size of the dataset\n",
    "print('Number of files in the training set: ', len(os.listdir('./training20_set')))\n",
    "print('Number of files in the test set: ', len(os.listdir('./testing20_set')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
